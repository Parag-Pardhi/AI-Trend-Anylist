{"file_contents":{"app.py":{"content":"import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport yfinance as yf\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import custom modules\nfrom data_collector import DataCollector\nfrom forecasting_models import TimeSeriesForecaster\nfrom clustering_analysis import CustomerSegmentation\nfrom sentiment_analyzer import MarketSentimentAnalyzer\nfrom utils.visualization import create_trend_chart, create_cluster_visualization\nfrom utils.data_preprocessor import DataPreprocessor\n\n# Configure Streamlit page\nst.set_page_config(\n    page_title=\"ðŸš€ AI Market Trend Analysis\",\n    page_icon=\"ðŸ“Š\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# Custom CSS for better styling\nst.markdown(\"\"\"\n    <style>\n    .main-header {\n        font-size: 3rem;\n        font-weight: bold;\n        text-align: center;\n        color: #FF6B6B;\n        margin-bottom: 2rem;\n    }\n    .metric-card {\n        background-color: #f0f2f6;\n        padding: 1rem;\n        border-radius: 0.5rem;\n        border-left: 4px solid #FF6B6B;\n    }\n    </style>\n\"\"\", unsafe_allow_html=True)\n\n# Initialize session state\nif 'data_loaded' not in st.session_state:\n    st.session_state.data_loaded = False\nif 'analysis_complete' not in st.session_state:\n    st.session_state.analysis_complete = False\n\n# Main title\nst.markdown('<h1 class=\"main-header\">ðŸš€ AI Market Trend Analysis System</h1>', unsafe_allow_html=True)\nst.markdown(\"**Advanced AI-powered market analysis with forecasting, clustering, and sentiment insights**\")\nst.markdown(\"---\")\n\n# Sidebar for navigation\nst.sidebar.title(\"ðŸ“Š Navigation\")\npage = st.sidebar.selectbox(\n    \"Choose Analysis Type\",\n    [\"ðŸ  Home\", \"ðŸ“ˆ Market Forecasting\", \"ðŸ‘¥ Customer Segmentation\", \"ðŸ’­ Sentiment Analysis\", \"ðŸ“Š Complete Dashboard\"]\n)\n\n# Initialize components\n@st.cache_resource\ndef load_components():\n    \"\"\"Load and cache all AI components\"\"\"\n    data_collector = DataCollector()\n    forecaster = TimeSeriesForecaster()\n    segmentation = CustomerSegmentation()\n    sentiment_analyzer = MarketSentimentAnalyzer()\n    preprocessor = DataPreprocessor()\n    return data_collector, forecaster, segmentation, sentiment_analyzer, preprocessor\n\ntry:\n    data_collector, forecaster, segmentation, sentiment_analyzer, preprocessor = load_components()\n    st.sidebar.success(\"âœ… AI Components Loaded!\")\nexcept Exception as e:\n    st.sidebar.error(f\"âŒ Error loading components: {str(e)}\")\n    st.stop()\n\n# Home page\nif page == \"ðŸ  Home\":\n    st.header(\"Welcome to AI Market Trend Analysis\")\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.info(\"\"\"\n        ### ðŸ“ˆ Market Forecasting\n        - Time series analysis using Prophet & ARIMA\n        - Demand prediction and trend identification\n        - Sales forecasting with confidence intervals\n        \"\"\")\n    \n    with col2:\n        st.info(\"\"\"\n        ### ðŸ‘¥ Customer Segmentation\n        - K-Means clustering analysis\n        - Behavioral pattern identification\n        - RFM analysis for customer value\n        \"\"\")\n    \n    with col3:\n        st.info(\"\"\"\n        ### ðŸ’­ Sentiment Analysis\n        - Market sentiment from social data\n        - Product review analysis\n        - Brand perception insights\n        \"\"\")\n    \n    # Sample data upload section\n    st.subheader(\"ðŸ“ Data Upload & Management\")\n    \n    col_upload1, col_upload2 = st.columns(2)\n    \n    with col_upload1:\n        st.markdown(\"#### Upload Your Data\")\n        uploaded_file = st.file_uploader(\n            \"Choose CSV file\",\n            type=['csv'],\n            help=\"Upload your market data (sales, customer, product information)\"\n        )\n        \n        if uploaded_file is not None:\n            try:\n                df = pd.read_csv(uploaded_file)\n                st.success(f\"âœ… Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n                st.session_state.uploaded_data = df\n                st.session_state.data_loaded = True\n                \n                with st.expander(\"Preview Data\"):\n                    st.dataframe(df.head())\n                    \n            except Exception as e:\n                st.error(f\"Error loading data: {str(e)}\")\n    \n    with col_upload2:\n        st.markdown(\"#### Or Use Sample Dataset\")\n        if st.button(\"ðŸ“Š Load Sample Retail Data\", type=\"primary\"):\n            try:\n                sample_data = data_collector.load_sample_data()\n                st.session_state.sample_data = sample_data\n                st.session_state.data_loaded = True\n                st.success(\"âœ… Sample data loaded successfully!\")\n                \n                with st.expander(\"Preview Sample Data\"):\n                    st.dataframe(sample_data.head())\n                    \n            except Exception as e:\n                st.error(f\"Error loading sample data: {str(e)}\")\n    \n    # Market data from APIs\n    st.subheader(\"ðŸŒ Real-time Market Data\")\n    \n    col_api1, col_api2 = st.columns(2)\n    \n    with col_api1:\n        st.markdown(\"#### Stock Market Data\")\n        stock_symbol = st.text_input(\"Enter stock symbol (e.g., AAPL, TSLA)\", value=\"AAPL\")\n        if st.button(\"ðŸ“ˆ Fetch Stock Data\"):\n            try:\n                with st.spinner(\"Fetching data...\"):\n                    stock_data = data_collector.get_stock_data(stock_symbol)\n                    st.session_state.stock_data = stock_data\n                    st.success(f\"âœ… {stock_symbol} data fetched!\")\n                    \n                    # Quick visualization\n                    fig = px.line(stock_data.reset_index(), \n                                x='Date', y='Close',\n                                title=f\"{stock_symbol} Stock Price Trend\")\n                    st.plotly_chart(fig, use_container_width=True)\n                    \n            except Exception as e:\n                st.error(f\"Error fetching stock data: {str(e)}\")\n    \n    with col_api2:\n        st.markdown(\"#### Economic Indicators\")\n        if st.button(\"ðŸ“Š Load Economic Data\"):\n            try:\n                with st.spinner(\"Loading economic indicators...\"):\n                    economic_data = data_collector.get_economic_indicators()\n                    st.session_state.economic_data = economic_data\n                    st.success(\"âœ… Economic data loaded!\")\n                    \n                    # Show key metrics\n                    if not economic_data.empty:\n                        metrics = economic_data.iloc[-1]\n                        col_m1, col_m2, col_m3 = st.columns(3)\n                        \n                        with col_m1:\n                            st.metric(\"GDP Growth\", f\"{metrics.get('GDP_Growth', 0):.2f}%\")\n                        with col_m2:\n                            st.metric(\"Inflation Rate\", f\"{metrics.get('Inflation', 0):.2f}%\")\n                        with col_m3:\n                            st.metric(\"Unemployment\", f\"{metrics.get('Unemployment', 0):.2f}%\")\n                            \n            except Exception as e:\n                st.error(f\"Error loading economic data: {str(e)}\")\n\n# Market Forecasting page\nelif page == \"ðŸ“ˆ Market Forecasting\":\n    st.header(\"ðŸ“ˆ Market Forecasting & Trend Analysis\")\n    \n    if not st.session_state.data_loaded:\n        st.warning(\"âš ï¸ Please load data from the Home page first!\")\n        st.stop()\n    \n    # Get available data\n    data_source = st.selectbox(\n        \"Select data source:\",\n        [\"Sample Data\", \"Uploaded Data\", \"Stock Data\"]\n    )\n    \n    if data_source == \"Sample Data\" and 'sample_data' in st.session_state:\n        df = st.session_state.sample_data\n    elif data_source == \"Uploaded Data\" and 'uploaded_data' in st.session_state:\n        df = st.session_state.uploaded_data\n    elif data_source == \"Stock Data\" and 'stock_data' in st.session_state:\n        df = st.session_state.stock_data\n    else:\n        st.error(\"Selected data source not available!\")\n        st.stop()\n    \n    # Preprocessing\n    st.subheader(\"ðŸ”§ Data Preprocessing\")\n    with st.expander(\"Data Preparation Steps\"):\n        processed_data = preprocessor.prepare_time_series_data(df)\n        st.success(\"âœ… Data preprocessing completed\")\n        \n        col_info1, col_info2, col_info3 = st.columns(3)\n        with col_info1:\n            st.metric(\"Total Records\", len(processed_data))\n        with col_info2:\n            st.metric(\"Date Range\", f\"{processed_data.index.min().strftime('%Y-%m-%d')} to {processed_data.index.max().strftime('%Y-%m-%d')}\")\n        with col_info3:\n            st.metric(\"Missing Values\", processed_data.isnull().sum().sum())\n    \n    # Model selection and parameters\n    st.subheader(\"ðŸ¤– Forecasting Configuration\")\n    \n    col_model1, col_model2 = st.columns(2)\n    \n    with col_model1:\n        model_type = st.selectbox(\n            \"Select forecasting model:\",\n            [\"Prophet\", \"ARIMA\", \"Linear Regression\", \"Random Forest\"]\n        )\n        \n        forecast_periods = st.slider(\"Forecast periods (days)\", 30, 365, 90)\n        \n    with col_model2:\n        target_column = st.selectbox(\n            \"Select target variable:\",\n            processed_data.select_dtypes(include=[np.number]).columns.tolist()\n        )\n        \n        include_seasonality = st.checkbox(\"Include seasonality\", value=True)\n    \n    # Run forecasting\n    if st.button(\"ðŸš€ Run Forecasting Analysis\", type=\"primary\"):\n        try:\n            with st.spinner(\"Training forecasting model...\"):\n                # Prepare data for forecasting\n                y = processed_data[target_column]\n                \n                # Generate forecast\n                if model_type == \"Prophet\":\n                    forecast_result = forecaster.prophet_forecast(y, forecast_periods, include_seasonality)\n                elif model_type == \"ARIMA\":\n                    forecast_result = forecaster.arima_forecast(y, forecast_periods)\n                elif model_type == \"Linear Regression\":\n                    forecast_result = forecaster.linear_forecast(y, forecast_periods)\n                else:  # Random Forest\n                    forecast_result = forecaster.random_forest_forecast(y, forecast_periods)\n                \n                st.session_state.forecast_result = forecast_result\n                st.success(\"âœ… Forecasting completed!\")\n                \n                # Display results\n                st.subheader(\"ðŸ“Š Forecasting Results\")\n                \n                # Metrics\n                col_metric1, col_metric2, col_metric3, col_metric4 = st.columns(4)\n                \n                with col_metric1:\n                    st.metric(\"Model Type\", model_type)\n                with col_metric2:\n                    st.metric(\"MAPE\", f\"{forecast_result['metrics']['mape']:.2f}%\")\n                with col_metric3:\n                    st.metric(\"RMSE\", f\"{forecast_result['metrics']['rmse']:.2f}\")\n                with col_metric4:\n                    trend = \"ðŸ“ˆ Upward\" if forecast_result['forecast'].iloc[-1] > forecast_result['forecast'].iloc[0] else \"ðŸ“‰ Downward\"\n                    st.metric(\"Trend Direction\", trend)\n                \n                # Visualization\n                fig = create_trend_chart(\n                    historical_data=y,\n                    forecast_data=forecast_result['forecast'],\n                    title=f\"{target_column} Forecast - {model_type}\"\n                )\n                st.plotly_chart(fig, use_container_width=True)\n                \n                # Insights\n                st.subheader(\"ðŸ’¡ Key Insights\")\n                \n                insights = forecaster.generate_insights(y, forecast_result['forecast'])\n                for insight in insights:\n                    st.info(f\"ðŸ“ {insight}\")\n                \n                # Download forecast\n                csv_data = pd.concat([\n                    pd.DataFrame({'Type': 'Historical', 'Date': y.index, 'Value': y.values}),\n                    pd.DataFrame({'Type': 'Forecast', 'Date': forecast_result['forecast'].index, 'Value': forecast_result['forecast'].values})\n                ])\n                \n                st.download_button(\n                    label=\"ðŸ“¥ Download Forecast Data\",\n                    data=csv_data.to_csv(index=False).encode('utf-8'),\n                    file_name=f\"forecast_{model_type}_{datetime.now().strftime('%Y%m%d')}.csv\",\n                    mime=\"text/csv\"\n                )\n                \n        except Exception as e:\n            st.error(f\"Error in forecasting: {str(e)}\")\n\n# Customer Segmentation page\nelif page == \"ðŸ‘¥ Customer Segmentation\":\n    st.header(\"ðŸ‘¥ Customer Segmentation Analysis\")\n    \n    if not st.session_state.data_loaded:\n        st.warning(\"âš ï¸ Please load data from the Home page first!\")\n        st.stop()\n    \n    # Get available data\n    if 'sample_data' in st.session_state:\n        df = st.session_state.sample_data\n    elif 'uploaded_data' in st.session_state:\n        df = st.session_state.uploaded_data\n    else:\n        st.error(\"No customer data available!\")\n        st.stop()\n    \n    st.subheader(\"ðŸ”§ Segmentation Configuration\")\n    \n    col_seg1, col_seg2 = st.columns(2)\n    \n    with col_seg1:\n        segmentation_type = st.selectbox(\n            \"Segmentation approach:\",\n            [\"RFM Analysis\", \"Behavioral Clustering\", \"Demographic Clustering\"]\n        )\n        \n        n_clusters = st.slider(\"Number of clusters\", 2, 10, 5)\n    \n    with col_seg2:\n        if segmentation_type == \"RFM Analysis\":\n            st.info(\"ðŸ“Š RFM Analysis segments customers based on:\\n- Recency: How recently they purchased\\n- Frequency: How often they purchase\\n- Monetary: How much they spend\")\n        elif segmentation_type == \"Behavioral Clustering\":\n            st.info(\"ðŸŽ¯ Behavioral clustering groups customers by:\\n- Purchase patterns\\n- Product preferences\\n- Seasonal behavior\")\n        else:\n            st.info(\"ðŸ‘¤ Demographic clustering groups by:\\n- Age groups\\n- Geographic location\\n- Income levels\")\n    \n    # Feature selection\n    st.subheader(\"ðŸŽ›ï¸ Feature Selection\")\n    \n    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n    \n    col_feat1, col_feat2 = st.columns(2)\n    \n    with col_feat1:\n        selected_numeric = st.multiselect(\n            \"Select numeric features:\",\n            numeric_columns,\n            default=numeric_columns[:3] if len(numeric_columns) >= 3 else numeric_columns\n        )\n    \n    with col_feat2:\n        selected_categorical = st.multiselect(\n            \"Select categorical features (optional):\",\n            categorical_columns,\n            default=[]\n        )\n    \n    if st.button(\"ðŸŽ¯ Run Customer Segmentation\", type=\"primary\"):\n        try:\n            with st.spinner(\"Performing customer segmentation...\"):\n                # Prepare features\n                features = selected_numeric + selected_categorical\n                \n                if not features:\n                    st.error(\"Please select at least one feature!\")\n                    st.stop()\n                \n                # Run segmentation\n                if segmentation_type == \"RFM Analysis\":\n                    result = segmentation.rfm_analysis(df, n_clusters)\n                elif segmentation_type == \"Behavioral Clustering\":\n                    result = segmentation.behavioral_clustering(df[features], n_clusters)\n                else:\n                    result = segmentation.demographic_clustering(df[features], n_clusters)\n                \n                st.session_state.segmentation_result = result\n                st.success(\"âœ… Customer segmentation completed!\")\n                \n                # Display results\n                st.subheader(\"ðŸ“Š Segmentation Results\")\n                \n                # Cluster distribution\n                cluster_counts = result['labels'].value_counts().sort_index()\n                \n                col_dist1, col_dist2 = st.columns(2)\n                \n                with col_dist1:\n                    fig_pie = px.pie(\n                        values=cluster_counts.values,\n                        names=[f\"Cluster {i}\" for i in cluster_counts.index],\n                        title=\"Customer Distribution by Cluster\"\n                    )\n                    st.plotly_chart(fig_pie, use_container_width=True)\n                \n                with col_dist2:\n                    st.subheader(\"ðŸ“ˆ Cluster Statistics\")\n                    for cluster_id, count in cluster_counts.items():\n                        percentage = (count / len(result['labels'])) * 100\n                        st.metric(f\"Cluster {cluster_id}\", f\"{count} customers ({percentage:.1f}%)\")\n                \n                # Cluster visualization\n                if len(selected_numeric) >= 2:\n                    fig_scatter = create_cluster_visualization(\n                        df[selected_numeric[:2]], \n                        result['labels'],\n                        selected_numeric[:2]\n                    )\n                    st.plotly_chart(fig_scatter, use_container_width=True)\n                \n                # Cluster profiles\n                st.subheader(\"ðŸ‘¤ Cluster Profiles\")\n                \n                for cluster_id in sorted(result['labels'].unique()):\n                    with st.expander(f\"Cluster {cluster_id} Profile\"):\n                        cluster_data = df[result['labels'] == cluster_id]\n                        \n                        col_prof1, col_prof2 = st.columns(2)\n                        \n                        with col_prof1:\n                            st.write(\"**Numeric Features:**\")\n                            for col in selected_numeric:\n                                if col in cluster_data.columns:\n                                    mean_val = cluster_data[col].mean()\n                                    st.write(f\"- {col}: {mean_val:.2f} (avg)\")\n                        \n                        with col_prof2:\n                            st.write(\"**Categorical Features:**\")\n                            for col in selected_categorical:\n                                if col in cluster_data.columns:\n                                    mode_val = cluster_data[col].mode().iloc[0] if not cluster_data[col].mode().empty else \"N/A\"\n                                    st.write(f\"- {col}: {mode_val} (most common)\")\n                \n                # Business insights\n                st.subheader(\"ðŸ’¡ Business Insights\")\n                insights = segmentation.generate_insights(result, df)\n                for insight in insights:\n                    st.info(f\"ðŸ“ {insight}\")\n                \n                # Download segmentation results\n                result_df = df.copy()\n                result_df['Cluster'] = result['labels']\n                \n                st.download_button(\n                    label=\"ðŸ“¥ Download Segmentation Results\",\n                    data=result_df.to_csv(index=False),\n                    file_name=f\"customer_segmentation_{datetime.now().strftime('%Y%m%d')}.csv\",\n                    mime=\"text/csv\"\n                )\n                \n        except Exception as e:\n            st.error(f\"Error in customer segmentation: {str(e)}\")\n\n# Sentiment Analysis page\nelif page == \"ðŸ’­ Sentiment Analysis\":\n    st.header(\"ðŸ’­ Market Sentiment Analysis\")\n    \n    st.subheader(\"ðŸ“ Text Input for Sentiment Analysis\")\n    \n    # Single text analysis\n    col_sent1, col_sent2 = st.columns([2, 1])\n    \n    with col_sent1:\n        text_input = st.text_area(\n            \"Enter text for sentiment analysis:\",\n            height=120,\n            placeholder=\"Enter product reviews, social media posts, or market commentary...\",\n            help=\"Analyze sentiment of market-related text\"\n        )\n        \n        if st.button(\"ðŸ” Analyze Sentiment\", type=\"primary\"):\n            if text_input.strip():\n                try:\n                    with st.spinner(\"Analyzing sentiment...\"):\n                        result = sentiment_analyzer.analyze_text(text_input)\n                        \n                        st.subheader(\"ðŸ“Š Sentiment Analysis Results\")\n                        \n                        col_res1, col_res2, col_res3 = st.columns(3)\n                        \n                        with col_res1:\n                            emoji = \"ðŸ˜Š\" if result['sentiment'] == \"POSITIVE\" else \"ðŸ˜ž\" if result['sentiment'] == \"NEGATIVE\" else \"ðŸ˜\"\n                            st.success(f\"{emoji} **{result['sentiment']}**\")\n                        \n                        with col_res2:\n                            st.metric(\"Confidence\", f\"{result['confidence']:.1%}\")\n                        \n                        with col_res3:\n                            st.metric(\"Processing Time\", f\"{result.get('processing_time', 0):.2f}s\")\n                        \n                        # Sentiment distribution\n                        if 'scores' in result:\n                            st.subheader(\"ðŸ“ˆ Sentiment Scores\")\n                            scores_df = pd.DataFrame([result['scores']])\n                            fig_bar = px.bar(\n                                x=list(result['scores'].keys()),\n                                y=list(result['scores'].values()),\n                                title=\"Sentiment Score Distribution\"\n                            )\n                            st.plotly_chart(fig_bar, use_container_width=True)\n                        \n                        # Explanation\n                        st.subheader(\"ðŸ’¡ Analysis Explanation\")\n                        explanation = sentiment_analyzer.explain_sentiment(text_input, result)\n                        st.info(explanation)\n                        \n                except Exception as e:\n                    st.error(f\"Error analyzing sentiment: {str(e)}\")\n            else:\n                st.warning(\"Please enter some text to analyze!\")\n    \n    with col_sent2:\n        st.subheader(\"ðŸ“‹ Quick Examples\")\n        examples = [\n            (\"Positive Review\", \"This product exceeded my expectations! Amazing quality and fast delivery. Highly recommend!\"),\n            (\"Negative Review\", \"Terrible experience. Product arrived damaged and customer service was unhelpful.\"),\n            (\"Mixed Review\", \"The product is okay, nothing special. Price is fair but could be better quality.\"),\n            (\"Market Bullish\", \"Strong earnings report shows positive market trends. Investors are optimistic about growth.\"),\n            (\"Market Bearish\", \"Economic uncertainty and declining sales indicate potential market downturn ahead.\")\n        ]\n        \n        for label, example in examples:\n            if st.button(label, key=f\"example_{label}\"):\n                st.session_state.example_text = example\n                st.rerun()\n        \n        if 'example_text' in st.session_state:\n            st.text_area(\"Selected example:\", value=st.session_state.example_text, height=100, disabled=True)\n    \n    # Batch sentiment analysis\n    st.markdown(\"---\")\n    st.subheader(\"ðŸ“Š Batch Sentiment Analysis\")\n    \n    col_batch1, col_batch2 = st.columns(2)\n    \n    with col_batch1:\n        batch_text = st.text_area(\n            \"Enter multiple texts (one per line):\",\n            height=150,\n            placeholder=\"\"\"Great product, love it!\nPoor quality, not worth the money.\nAverage experience, nothing special.\nOutstanding customer service!\nDisappointed with the delivery delay.\"\"\"\n        )\n        \n        if st.button(\"ðŸ“Š Analyze All Texts\"):\n            if batch_text.strip():\n                try:\n                    texts = [line.strip() for line in batch_text.split('\\n') if line.strip()]\n                    \n                    if texts:\n                        with st.spinner(\"Analyzing multiple texts...\"):\n                            batch_results = sentiment_analyzer.analyze_batch(texts)\n                            \n                            st.subheader(\"ðŸ“ˆ Batch Analysis Results\")\n                            \n                            # Summary statistics\n                            positive_count = sum(1 for r in batch_results if r['sentiment'] == 'POSITIVE')\n                            negative_count = sum(1 for r in batch_results if r['sentiment'] == 'NEGATIVE')\n                            neutral_count = len(batch_results) - positive_count - negative_count\n                            avg_confidence = np.mean([r['confidence'] for r in batch_results])\n                            \n                            col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)\n                            \n                            with col_stat1:\n                                st.metric(\"Total Texts\", len(texts))\n                            with col_stat2:\n                                st.metric(\"ðŸ˜Š Positive\", f\"{positive_count} ({positive_count/len(texts)*100:.1f}%)\")\n                            with col_stat3:\n                                st.metric(\"ðŸ˜ž Negative\", f\"{negative_count} ({negative_count/len(texts)*100:.1f}%)\")\n                            with col_stat4:\n                                st.metric(\"Avg Confidence\", f\"{avg_confidence:.1%}\")\n                            \n                            # Results table\n                            results_df = pd.DataFrame([\n                                {\n                                    'Text': text[:50] + \"...\" if len(text) > 50 else text,\n                                    'Sentiment': result['sentiment'],\n                                    'Confidence': f\"{result['confidence']:.1%}\",\n                                    'Full_Text': text\n                                }\n                                for text, result in zip(texts, batch_results)\n                            ])\n                            \n                            st.dataframe(results_df[['Text', 'Sentiment', 'Confidence']], use_container_width=True)\n                            \n                            # Download results\n                            csv_data = results_df.to_csv(index=False)\n                            st.download_button(\n                                label=\"ðŸ“¥ Download Sentiment Results\",\n                                data=csv_data,\n                                file_name=f\"sentiment_analysis_{datetime.now().strftime('%Y%m%d')}.csv\",\n                                mime=\"text/csv\"\n                            )\n                            \n                except Exception as e:\n                    st.error(f\"Error in batch analysis: {str(e)}\")\n            else:\n                st.warning(\"Please enter some texts to analyze!\")\n    \n    with col_batch2:\n        st.info(\"\"\"\n        ### ðŸ’¡ Sentiment Analysis Features:\n        \n        **ðŸŽ¯ Capabilities:**\n        - Real-time sentiment classification\n        - Confidence score assessment\n        - Batch processing support\n        - Market-specific language understanding\n        \n        **ðŸ“Š Use Cases:**\n        - Product review analysis\n        - Social media monitoring\n        - Market sentiment tracking\n        - Brand perception analysis\n        \n        **ðŸ” Technical Details:**\n        - Transformer-based models\n        - Multi-class sentiment classification\n        - Confidence intervals\n        - Explainable results\n        \"\"\")\n\n# Complete Dashboard page\nelif page == \"ðŸ“Š Complete Dashboard\":\n    st.header(\"ðŸ“Š Complete Market Analysis Dashboard\")\n    \n    if not st.session_state.data_loaded:\n        st.warning(\"âš ï¸ Please load data and run analyses from other pages first!\")\n        st.stop()\n    \n    # Dashboard summary\n    st.subheader(\"ðŸŽ¯ Analysis Summary\")\n    \n    col_dash1, col_dash2, col_dash3, col_dash4 = st.columns(4)\n    \n    with col_dash1:\n        data_status = \"âœ… Loaded\" if st.session_state.data_loaded else \"âŒ Not loaded\"\n        st.metric(\"Data Status\", data_status)\n    \n    with col_dash2:\n        forecast_status = \"âœ… Complete\" if 'forecast_result' in st.session_state else \"â³ Pending\"\n        st.metric(\"Forecasting\", forecast_status)\n    \n    with col_dash3:\n        segmentation_status = \"âœ… Complete\" if 'segmentation_result' in st.session_state else \"â³ Pending\"\n        st.metric(\"Segmentation\", segmentation_status)\n    \n    with col_dash4:\n        sentiment_status = \"âœ… Available\" if sentiment_analyzer else \"âŒ Error\"\n        st.metric(\"Sentiment Engine\", sentiment_status)\n    \n    # Integrated visualizations\n    if 'forecast_result' in st.session_state:\n        st.subheader(\"ðŸ“ˆ Forecasting Dashboard\")\n        \n        forecast_data = st.session_state.forecast_result\n        \n        # Create combined chart\n        fig = go.Figure()\n        \n        # Add historical data if available\n        if 'sample_data' in st.session_state:\n            historical = st.session_state.sample_data\n            if len(historical.select_dtypes(include=[np.number]).columns) > 0:\n                target_col = historical.select_dtypes(include=[np.number]).columns[0]\n                fig.add_trace(go.Scatter(\n                    x=historical.index if hasattr(historical.index, 'name') else range(len(historical)),\n                    y=historical[target_col] if target_col in historical.columns else historical.iloc[:, 0],\n                    mode='lines',\n                    name='Historical Data',\n                    line=dict(color='blue')\n                ))\n        \n        # Add forecast\n        fig.add_trace(go.Scatter(\n            x=forecast_data['forecast'].index,\n            y=forecast_data['forecast'].values,\n            mode='lines',\n            name='Forecast',\n            line=dict(color='red', dash='dash')\n        ))\n        \n        fig.update_layout(\n            title=\"Market Trend Forecast\",\n            xaxis_title=\"Time Period\",\n            yaxis_title=\"Value\",\n            height=400\n        )\n        \n        st.plotly_chart(fig, use_container_width=True)\n        \n        # Key metrics\n        col_metric1, col_metric2, col_metric3 = st.columns(3)\n        \n        with col_metric1:\n            st.metric(\"Forecast Accuracy (MAPE)\", f\"{forecast_data['metrics']['mape']:.2f}%\")\n        with col_metric2:\n            trend_direction = \"ðŸ“ˆ Upward\" if forecast_data['forecast'].iloc[-1] > forecast_data['forecast'].iloc[0] else \"ðŸ“‰ Downward\"\n            st.metric(\"Trend Direction\", trend_direction)\n        with col_metric3:\n            st.metric(\"Confidence Level\", \"High\" if forecast_data['metrics']['mape'] < 10 else \"Medium\")\n    \n    if 'segmentation_result' in st.session_state:\n        st.subheader(\"ðŸ‘¥ Customer Segmentation Overview\")\n        \n        segmentation_data = st.session_state.segmentation_result\n        cluster_counts = segmentation_data['labels'].value_counts().sort_index()\n        \n        col_seg_dash1, col_seg_dash2 = st.columns(2)\n        \n        with col_seg_dash1:\n            # Cluster distribution pie chart\n            fig_pie = px.pie(\n                values=cluster_counts.values,\n                names=[f\"Segment {i}\" for i in cluster_counts.index],\n                title=\"Customer Segmentation Distribution\"\n            )\n            st.plotly_chart(fig_pie, use_container_width=True)\n        \n        with col_seg_dash2:\n            # Cluster size metrics\n            st.markdown(\"**Segment Sizes:**\")\n            for cluster_id, count in cluster_counts.items():\n                percentage = (count / len(segmentation_data['labels'])) * 100\n                st.metric(f\"Segment {cluster_id}\", f\"{count} ({percentage:.1f}%)\")\n    \n    # Actionable insights section\n    st.subheader(\"ðŸ’¡ Actionable Business Insights\")\n    \n    insights_tabs = st.tabs([\"ðŸŽ¯ Strategic\", \"ðŸ“Š Operational\", \"ðŸ’° Financial\"])\n    \n    with insights_tabs[0]:\n        st.markdown(\"\"\"\n        ### Strategic Insights:\n        \"\"\")\n        \n        if 'forecast_result' in st.session_state:\n            forecast_trend = \"growth\" if st.session_state.forecast_result['forecast'].iloc[-1] > st.session_state.forecast_result['forecast'].iloc[0] else \"decline\"\n            st.info(f\"ðŸ“ˆ Market forecast shows {forecast_trend} trend - consider adjusting long-term strategy accordingly\")\n        \n        if 'segmentation_result' in st.session_state:\n            n_segments = len(st.session_state.segmentation_result['labels'].unique())\n            st.info(f\"ðŸ‘¥ {n_segments} distinct customer segments identified - opportunities for targeted marketing strategies\")\n        \n        st.info(\"ðŸŽ¯ AI-driven insights enable data-driven decision making and competitive advantage\")\n    \n    with insights_tabs[1]:\n        st.markdown(\"\"\"\n        ### Operational Recommendations:\n        \"\"\")\n        \n        st.success(\"ðŸ“¦ Optimize inventory levels based on demand forecasting\")\n        st.success(\"ðŸŽ¯ Implement segment-specific marketing campaigns\")\n        st.success(\"ðŸ“Š Monitor sentiment trends for proactive issue management\")\n        st.success(\"âš¡ Automate routine analysis tasks for efficiency\")\n    \n    with insights_tabs[2]:\n        st.markdown(\"\"\"\n        ### Financial Impact:\n        \"\"\")\n        \n        st.info(\"ðŸ’° Demand forecasting can reduce inventory costs by 10-30%\")\n        st.info(\"ðŸŽ¯ Customer segmentation can improve marketing ROI by 15-25%\")\n        st.info(\"ðŸ“ˆ Sentiment analysis can prevent revenue loss from negative trends\")\n        st.info(\"ðŸ” Overall AI implementation can increase profitability by 5-15%\")\n    \n    # Export comprehensive report\n    st.subheader(\"ðŸ“‹ Export Analysis Report\")\n    \n    if st.button(\"ðŸ“¥ Generate Complete Report\", type=\"primary\"):\n        try:\n            # Compile all results into comprehensive report\n            report_data = {\n                'timestamp': datetime.now().isoformat(),\n                'data_status': st.session_state.data_loaded,\n                'forecast_available': 'forecast_result' in st.session_state,\n                'segmentation_available': 'segmentation_result' in st.session_state\n            }\n            \n            # Add forecast data if available\n            if 'forecast_result' in st.session_state:\n                report_data['forecast_metrics'] = st.session_state.forecast_result['metrics']\n            \n            # Add segmentation data if available\n            if 'segmentation_result' in st.session_state:\n                report_data['segmentation_summary'] = {\n                    'n_clusters': len(st.session_state.segmentation_result['labels'].unique()),\n                    'cluster_distribution': st.session_state.segmentation_result['labels'].value_counts().to_dict()\n                }\n            \n            report_json = pd.Series(report_data).to_json()\n            \n            st.download_button(\n                label=\"ðŸ“„ Download JSON Report\",\n                data=report_json,\n                file_name=f\"market_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n                mime=\"application/json\"\n            )\n            \n            st.success(\"âœ… Report generated successfully!\")\n            \n        except Exception as e:\n            st.error(f\"Error generating report: {str(e)}\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"\"\"\n### ðŸŽ“ AI Market Trend Analysis System\n\n**ðŸ”¬ Built with:** Python, Streamlit, Prophet, Scikit-learn, HuggingFace Transformers  \n**ðŸ“Š Features:** Time series forecasting, customer segmentation, sentiment analysis  \n**ðŸŽ¯ Purpose:** Academic research and business intelligence  \n**ðŸ“… Created:** August 2025\n\n*This system demonstrates advanced AI techniques for market analysis and business intelligence.*\n\"\"\")\n\n# Sidebar info\nst.sidebar.markdown(\"---\")\nst.sidebar.markdown(\"\"\"\n### ðŸ“Š System Status\n\n**ðŸ”§ Components:**\n- âœ… Data Collection\n- âœ… Forecasting Engine\n- âœ… Clustering Analysis\n- âœ… Sentiment Analysis\n- âœ… Visualization Engine\n\n**ðŸ“ˆ Capabilities:**\n- Prophet & ARIMA forecasting\n- K-Means clustering\n- Transformer sentiment analysis\n- Interactive dashboards\n- Explainable AI with SHAP\n\n**ðŸŽ¯ Academic Focus:**\n- Market trend prediction\n- Customer behavior analysis\n- Business intelligence\n- AI model evaluation\n\"\"\")\n","size_bytes":36387},"clustering_analysis.py":{"content":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass CustomerSegmentation:\n    \"\"\"\n    Customer segmentation and clustering analysis module.\n    Supports RFM analysis, behavioral clustering, and demographic segmentation.\n    \"\"\"\n    \n    def __init__(self):\n        self.scalers = {}\n        self.encoders = {}\n        self.models = {}\n        \n    def rfm_analysis(self, df, n_clusters=5):\n        \"\"\"\n        Perform RFM (Recency, Frequency, Monetary) analysis.\n        \n        Args:\n            df (pd.DataFrame): Transaction data\n            n_clusters (int): Number of customer segments\n        \n        Returns:\n            dict: RFM analysis results\n        \"\"\"\n        try:\n            # Create RFM features if not present\n            rfm_data = self._create_rfm_features(df)\n            \n            if rfm_data is None or rfm_data.empty:\n                raise ValueError(\"Could not create RFM features from data\")\n            \n            # Scale RFM features\n            scaler = StandardScaler()\n            rfm_scaled = scaler.fit_transform(rfm_data[['Recency', 'Frequency', 'Monetary']])\n            \n            # Perform K-means clustering\n            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n            cluster_labels = kmeans.fit_predict(rfm_scaled)\n            \n            # Add cluster labels to RFM data\n            rfm_data['Cluster'] = cluster_labels\n            \n            # Calculate cluster statistics\n            cluster_stats = self._calculate_cluster_stats(rfm_data, ['Recency', 'Frequency', 'Monetary'])\n            \n            # Calculate clustering metrics\n            metrics = self._calculate_clustering_metrics(rfm_scaled, cluster_labels)\n            \n            # Generate segment descriptions\n            segment_descriptions = self._generate_rfm_segments(rfm_data)\n            \n            # Store model and scaler\n            self.models['rfm'] = kmeans\n            self.scalers['rfm'] = scaler\n            \n            return {\n                'data': rfm_data,\n                'labels': pd.Series(cluster_labels, index=rfm_data.index),\n                'cluster_stats': cluster_stats,\n                'metrics': metrics,\n                'segment_descriptions': segment_descriptions,\n                'model': kmeans,\n                'scaler': scaler\n            }\n            \n        except Exception as e:\n            print(f\"Error in RFM analysis: {e}\")\n            return self._fallback_clustering(df, n_clusters)\n    \n    def behavioral_clustering(self, df, n_clusters=5, algorithm='kmeans'):\n        \"\"\"\n        Perform behavioral clustering based on customer behavior patterns.\n        \n        Args:\n            df (pd.DataFrame): Customer behavior data\n            n_clusters (int): Number of clusters\n            algorithm (str): Clustering algorithm ('kmeans', 'dbscan', 'hierarchical')\n        \n        Returns:\n            dict: Clustering results\n        \"\"\"\n        try:\n            # Prepare behavioral features\n            behavior_features = self._prepare_behavioral_features(df)\n            \n            if behavior_features is None or behavior_features.empty:\n                raise ValueError(\"Could not prepare behavioral features\")\n            \n            # Scale features\n            scaler = StandardScaler()\n            features_scaled = scaler.fit_transform(behavior_features)\n            \n            # Apply clustering algorithm\n            if algorithm == 'kmeans':\n                model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n                cluster_labels = model.fit_predict(features_scaled)\n            elif algorithm == 'dbscan':\n                model = DBSCAN(eps=0.5, min_samples=5)\n                cluster_labels = model.fit_predict(features_scaled)\n                n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n            else:  # hierarchical\n                model = AgglomerativeClustering(n_clusters=n_clusters)\n                cluster_labels = model.fit_predict(features_scaled)\n            \n            # Calculate cluster statistics\n            cluster_stats = self._calculate_cluster_stats(behavior_features, behavior_features.columns)\n            \n            # Calculate clustering metrics\n            if len(set(cluster_labels)) > 1:\n                metrics = self._calculate_clustering_metrics(features_scaled, cluster_labels)\n            else:\n                metrics = {'silhouette_score': 0, 'calinski_harabasz_score': 0, 'davies_bouldin_score': 0}\n            \n            # Store model and scaler\n            self.models['behavioral'] = model\n            self.scalers['behavioral'] = scaler\n            \n            return {\n                'data': behavior_features,\n                'labels': pd.Series(cluster_labels, index=behavior_features.index),\n                'cluster_stats': cluster_stats,\n                'metrics': metrics,\n                'n_clusters': n_clusters,\n                'algorithm': algorithm,\n                'model': model,\n                'scaler': scaler\n            }\n            \n        except Exception as e:\n            print(f\"Error in behavioral clustering: {e}\")\n            return self._fallback_clustering(df, n_clusters)\n    \n    def demographic_clustering(self, df, n_clusters=5):\n        \"\"\"\n        Perform demographic-based customer segmentation.\n        \n        Args:\n            df (pd.DataFrame): Customer demographic data\n            n_clusters (int): Number of clusters\n        \n        Returns:\n            dict: Clustering results\n        \"\"\"\n        try:\n            # Prepare demographic features\n            demo_features = self._prepare_demographic_features(df)\n            \n            if demo_features is None or demo_features.empty:\n                raise ValueError(\"Could not prepare demographic features\")\n            \n            # Scale numeric features\n            numeric_cols = demo_features.select_dtypes(include=[np.number]).columns\n            \n            if len(numeric_cols) > 0:\n                scaler = StandardScaler()\n                demo_features[numeric_cols] = scaler.fit_transform(demo_features[numeric_cols])\n                self.scalers['demographic'] = scaler\n            \n            # Perform clustering\n            features_for_clustering = demo_features.select_dtypes(include=[np.number])\n            \n            if features_for_clustering.empty:\n                raise ValueError(\"No numeric features available for clustering\")\n            \n            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n            cluster_labels = kmeans.fit_predict(features_for_clustering)\n            \n            # Calculate cluster statistics\n            cluster_stats = self._calculate_cluster_stats(demo_features, demo_features.columns)\n            \n            # Calculate clustering metrics\n            metrics = self._calculate_clustering_metrics(features_for_clustering.values, cluster_labels)\n            \n            # Store model\n            self.models['demographic'] = kmeans\n            \n            return {\n                'data': demo_features,\n                'labels': pd.Series(cluster_labels, index=demo_features.index),\n                'cluster_stats': cluster_stats,\n                'metrics': metrics,\n                'model': kmeans\n            }\n            \n        except Exception as e:\n            print(f\"Error in demographic clustering: {e}\")\n            return self._fallback_clustering(df, n_clusters)\n    \n    def _create_rfm_features(self, df):\n        \"\"\"Create RFM features from transaction data\"\"\"\n        try:\n            # Try to identify relevant columns\n            date_cols = df.select_dtypes(include=['datetime64']).columns\n            numeric_cols = df.select_dtypes(include=[np.number]).columns\n            \n            if len(date_cols) == 0:\n                # Try to find date columns in object type\n                for col in df.columns:\n                    if any(keyword in col.lower() for keyword in ['date', 'time', 'day']):\n                        try:\n                            df[col] = pd.to_datetime(df[col])\n                            date_cols = [col]\n                            break\n                        except:\n                            continue\n            \n            if len(date_cols) == 0:\n                # Create synthetic date column based on index\n                df['Date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='D')\n                date_cols = ['Date']\n            \n            # Identify customer, date, and monetary columns\n            date_col = date_cols[0]\n            \n            # Try to identify monetary column (sales, revenue, amount, etc.)\n            monetary_col = None\n            for col in numeric_cols:\n                if any(keyword in col.lower() for keyword in ['sale', 'revenue', 'amount', 'value', 'price', 'total']):\n                    monetary_col = col\n                    break\n            \n            if monetary_col is None and len(numeric_cols) > 0:\n                monetary_col = numeric_cols[0]  # Use first numeric column\n            \n            # Create customer ID if not present\n            if 'customer_id' not in df.columns and 'Customer' not in df.columns:\n                df['customer_id'] = range(len(df))\n                customer_col = 'customer_id'\n            else:\n                customer_col = 'customer_id' if 'customer_id' in df.columns else 'Customer'\n            \n            # Calculate RFM metrics\n            current_date = df[date_col].max()\n            \n            # Group by customer\n            customer_data = df.groupby(customer_col).agg({\n                date_col: ['max', 'count'],\n                monetary_col: ['sum', 'mean'] if monetary_col else lambda x: len(x)\n            }).reset_index()\n            \n            # Flatten column names\n            customer_data.columns = [customer_col, 'LastPurchaseDate', 'PurchaseCount', 'TotalSpent', 'AvgSpent']\n            \n            # Calculate RFM metrics\n            customer_data['Recency'] = (current_date - customer_data['LastPurchaseDate']).dt.days\n            customer_data['Frequency'] = customer_data['PurchaseCount']\n            customer_data['Monetary'] = customer_data['TotalSpent']\n            \n            # Set customer as index\n            customer_data.set_index(customer_col, inplace=True)\n            \n            return customer_data[['Recency', 'Frequency', 'Monetary']]\n            \n        except Exception as e:\n            print(f\"Error creating RFM features: {e}\")\n            # Create synthetic RFM data\n            n_customers = min(len(df), 1000)\n            synthetic_rfm = pd.DataFrame({\n                'Recency': np.random.exponential(30, n_customers),\n                'Frequency': np.random.poisson(5, n_customers) + 1,\n                'Monetary': np.random.lognormal(5, 1, n_customers)\n            })\n            return synthetic_rfm\n    \n    def _prepare_behavioral_features(self, df):\n        \"\"\"Prepare behavioral features from customer data\"\"\"\n        try:\n            numeric_cols = df.select_dtypes(include=[np.number]).columns\n            \n            if len(numeric_cols) == 0:\n                raise ValueError(\"No numeric columns for behavioral analysis\")\n            \n            # Select relevant behavioral columns\n            behavioral_features = df[numeric_cols].copy()\n            \n            # Create additional behavioral metrics if possible\n            if len(behavioral_features.columns) >= 2:\n                # Create ratios and interactions\n                cols = behavioral_features.columns[:2]\n                behavioral_features[f'{cols[0]}_to_{cols[1]}_ratio'] = (\n                    behavioral_features[cols[0]] / (behavioral_features[cols[1]] + 1)\n                )\n            \n            # Handle missing values\n            behavioral_features.fillna(behavioral_features.median(), inplace=True)\n            \n            return behavioral_features\n            \n        except Exception as e:\n            print(f\"Error preparing behavioral features: {e}\")\n            return None\n    \n    def _prepare_demographic_features(self, df):\n        \"\"\"Prepare demographic features for clustering\"\"\"\n        try:\n            demo_features = df.copy()\n            \n            # Encode categorical variables\n            categorical_cols = demo_features.select_dtypes(include=['object']).columns\n            \n            for col in categorical_cols:\n                if demo_features[col].nunique() < 50:  # Only encode if reasonable number of categories\n                    le = LabelEncoder()\n                    demo_features[col] = le.fit_transform(demo_features[col].astype(str))\n                    self.encoders[col] = le\n                else:\n                    demo_features.drop(col, axis=1, inplace=True)\n            \n            # Handle missing values\n            numeric_cols = demo_features.select_dtypes(include=[np.number]).columns\n            demo_features[numeric_cols] = demo_features[numeric_cols].fillna(demo_features[numeric_cols].median())\n            \n            return demo_features\n            \n        except Exception as e:\n            print(f\"Error preparing demographic features: {e}\")\n            return None\n    \n    def _calculate_cluster_stats(self, data, columns):\n        \"\"\"Calculate statistics for each cluster\"\"\"\n        stats = {}\n        \n        for cluster in data['Cluster'].unique():\n            cluster_data = data[data['Cluster'] == cluster]\n            cluster_stats = {}\n            \n            for col in columns:\n                if col in cluster_data.columns and pd.api.types.is_numeric_dtype(cluster_data[col]):\n                    cluster_stats[col] = {\n                        'mean': cluster_data[col].mean(),\n                        'median': cluster_data[col].median(),\n                        'std': cluster_data[col].std(),\n                        'min': cluster_data[col].min(),\n                        'max': cluster_data[col].max(),\n                        'count': len(cluster_data)\n                    }\n                elif col in cluster_data.columns:\n                    cluster_stats[col] = {\n                        'mode': cluster_data[col].mode().iloc[0] if not cluster_data[col].mode().empty else 'N/A',\n                        'unique_count': cluster_data[col].nunique(),\n                        'count': len(cluster_data)\n                    }\n            \n            stats[cluster] = cluster_stats\n        \n        return stats\n    \n    def _calculate_clustering_metrics(self, X, labels):\n        \"\"\"Calculate clustering evaluation metrics\"\"\"\n        try:\n            unique_labels = np.unique(labels)\n            \n            if len(unique_labels) <= 1:\n                return {\n                    'silhouette_score': 0,\n                    'calinski_harabasz_score': 0,\n                    'davies_bouldin_score': float('inf')\n                }\n            \n            # Remove noise points for DBSCAN\n            if -1 in labels:\n                mask = labels != -1\n                X_clean = X[mask]\n                labels_clean = labels[mask]\n            else:\n                X_clean = X\n                labels_clean = labels\n            \n            if len(np.unique(labels_clean)) <= 1 or len(X_clean) < 2:\n                return {\n                    'silhouette_score': 0,\n                    'calinski_harabasz_score': 0,\n                    'davies_bouldin_score': float('inf')\n                }\n            \n            silhouette = silhouette_score(X_clean, labels_clean)\n            calinski_harabasz = calinski_harabasz_score(X_clean, labels_clean)\n            davies_bouldin = davies_bouldin_score(X_clean, labels_clean)\n            \n            return {\n                'silhouette_score': silhouette,\n                'calinski_harabasz_score': calinski_harabasz,\n                'davies_bouldin_score': davies_bouldin\n            }\n            \n        except Exception as e:\n            print(f\"Error calculating clustering metrics: {e}\")\n            return {\n                'silhouette_score': 0,\n                'calinski_harabasz_score': 0,\n                'davies_bouldin_score': float('inf')\n            }\n    \n    def _generate_rfm_segments(self, rfm_data):\n        \"\"\"Generate descriptive names for RFM segments\"\"\"\n        segment_descriptions = {}\n        \n        for cluster in rfm_data['Cluster'].unique():\n            cluster_data = rfm_data[rfm_data['Cluster'] == cluster]\n            \n            avg_recency = cluster_data['Recency'].mean()\n            avg_frequency = cluster_data['Frequency'].mean()\n            avg_monetary = cluster_data['Monetary'].mean()\n            \n            # Generate segment description\n            if avg_recency <= 30 and avg_frequency >= 5 and avg_monetary >= rfm_data['Monetary'].median():\n                description = \"Champions\"\n            elif avg_recency <= 30 and avg_frequency >= 3:\n                description = \"Loyal Customers\"\n            elif avg_recency <= 60 and avg_monetary >= rfm_data['Monetary'].median():\n                description = \"Potential Loyalists\"\n            elif avg_recency <= 30:\n                description = \"New Customers\"\n            elif avg_recency <= 90 and avg_frequency >= 2:\n                description = \"At Risk\"\n            elif avg_recency > 90 and avg_frequency >= 3:\n                description = \"Cannot Lose Them\"\n            elif avg_recency > 90:\n                description = \"Lost Customers\"\n            else:\n                description = \"Others\"\n            \n            segment_descriptions[cluster] = {\n                'name': description,\n                'avg_recency': avg_recency,\n                'avg_frequency': avg_frequency,\n                'avg_monetary': avg_monetary,\n                'size': len(cluster_data)\n            }\n        \n        return segment_descriptions\n    \n    def _fallback_clustering(self, df, n_clusters):\n        \"\"\"Fallback clustering when main methods fail\"\"\"\n        try:\n            # Use first few numeric columns\n            numeric_cols = df.select_dtypes(include=[np.number]).columns[:3]\n            \n            if len(numeric_cols) == 0:\n                # Create synthetic features\n                features = pd.DataFrame({\n                    'feature_1': np.random.normal(0, 1, len(df)),\n                    'feature_2': np.random.normal(0, 1, len(df)),\n                    'feature_3': np.random.normal(0, 1, len(df))\n                })\n            else:\n                features = df[numeric_cols].fillna(0)\n            \n            # Simple K-means clustering\n            scaler = StandardScaler()\n            features_scaled = scaler.fit_transform(features)\n            \n            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n            labels = kmeans.fit_predict(features_scaled)\n            \n            return {\n                'data': features,\n                'labels': pd.Series(labels, index=features.index),\n                'cluster_stats': {},\n                'metrics': {'silhouette_score': 0, 'calinski_harabasz_score': 0, 'davies_bouldin_score': 0},\n                'model': kmeans,\n                'scaler': scaler\n            }\n            \n        except Exception as e:\n            print(f\"Error in fallback clustering: {e}\")\n            # Return random assignments as last resort\n            labels = np.random.randint(0, n_clusters, len(df))\n            return {\n                'data': pd.DataFrame({'feature': range(len(df))}),\n                'labels': pd.Series(labels),\n                'cluster_stats': {},\n                'metrics': {'silhouette_score': 0, 'calinski_harabasz_score': 0, 'davies_bouldin_score': 0},\n                'model': None,\n                'scaler': None\n            }\n    \n    def generate_insights(self, clustering_result, original_data):\n        \"\"\"Generate business insights from clustering results\"\"\"\n        insights = []\n        \n        try:\n            labels = clustering_result['labels']\n            n_clusters = len(labels.unique())\n            \n            # Cluster size insights\n            cluster_sizes = labels.value_counts().sort_index()\n            largest_cluster = cluster_sizes.idxmax()\n            smallest_cluster = cluster_sizes.idxmin()\n            \n            insights.append(f\"ðŸ“Š Identified {n_clusters} distinct customer segments\")\n            insights.append(f\"ðŸŽ¯ Largest segment: Cluster {largest_cluster} ({cluster_sizes[largest_cluster]} customers, {cluster_sizes[largest_cluster]/len(labels)*100:.1f}%)\")\n            insights.append(f\"ðŸ“ Smallest segment: Cluster {smallest_cluster} ({cluster_sizes[smallest_cluster]} customers, {cluster_sizes[smallest_cluster]/len(labels)*100:.1f}%)\")\n            \n            # Balance insights\n            if cluster_sizes.max() / cluster_sizes.min() > 5:\n                insights.append(\"âš ï¸ High imbalance between segments - consider reviewing segmentation strategy\")\n            else:\n                insights.append(\"âœ… Well-balanced customer segments for targeted marketing\")\n            \n            # Clustering quality insights\n            if 'metrics' in clustering_result:\n                silhouette = clustering_result['metrics'].get('silhouette_score', 0)\n                \n                if silhouette > 0.7:\n                    insights.append(\"ðŸŽ¯ Excellent clustering quality - segments are well-separated\")\n                elif silhouette > 0.5:\n                    insights.append(\"ðŸ‘ Good clustering quality - segments show reasonable separation\")\n                elif silhouette > 0.25:\n                    insights.append(\"âš ï¸ Moderate clustering quality - consider adjusting number of segments\")\n                else:\n                    insights.append(\"âŒ Poor clustering quality - segments may not be meaningful\")\n            \n            # Business recommendations\n            insights.append(\"ðŸ’¼ Recommendation: Develop segment-specific marketing campaigns\")\n            insights.append(\"ðŸ“ˆ Recommendation: Monitor segment migration patterns over time\")\n            insights.append(\"ðŸŽ¯ Recommendation: Focus retention efforts on high-value segments\")\n            \n        except Exception as e:\n            insights.append(f\"âš ï¸ Error generating insights: {str(e)}\")\n        \n        return insights\n","size_bytes":22705},"data_collector.py":{"content":"import pandas as pd\nimport numpy as np\nimport yfinance as yf\nimport requests\nfrom datetime import datetime, timedelta\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass DataCollector:\n    \"\"\"\n    Data collection module for market trend analysis.\n    Handles various data sources including sample data, APIs, and web scraping.\n    \"\"\"\n    \n    def __init__(self):\n        self.api_key = os.getenv(\"API_KEY\", \"demo_key\")\n        self.cache = {}\n        # Indian stock symbols mapping\n        self.indian_stocks = {\n            'RELIANCE': 'RELIANCE.NS', 'TCS': 'TCS.NS', 'INFY': 'INFY.NS',\n            'HDFCBANK': 'HDFCBANK.NS', 'ICICIBANK': 'ICICIBANK.NS', 'HINDUNILVR': 'HINDUNILVR.NS',\n            'BHARTIARTL': 'BHARTIARTL.NS', 'ITC': 'ITC.NS', 'KOTAKBANK': 'KOTAKBANK.NS',\n            'LT': 'LT.NS', 'SBIN': 'SBIN.NS', 'ASIANPAINT': 'ASIANPAINT.NS',\n            'MARUTI': 'MARUTI.NS', 'BAJFINANCE': 'BAJFINANCE.NS', 'HCLTECH': 'HCLTECH.NS',\n            'WIPRO': 'WIPRO.NS', 'ULTRACEMCO': 'ULTRACEMCO.NS', 'TITAN': 'TITAN.NS',\n            'NESTLEIND': 'NESTLEIND.NS', 'POWERGRID': 'POWERGRID.NS', 'ADANIPORTS': 'ADANIPORTS.NS',\n            'AXISBANK': 'AXISBANK.NS', 'TATAMOTORS': 'TATAMOTORS.NS', 'TATASTEEL': 'TATASTEEL.NS',\n            'SUNPHARMA': 'SUNPHARMA.NS', 'ONGC': 'ONGC.NS', 'NTPC': 'NTPC.NS', 'TECHM': 'TECHM.NS'\n        }\n        \n        # Country economic indicators mapping\n        self.country_indicators = {\n            'US': ['GDP', 'Inflation', 'Unemployment', 'Interest_Rate'],\n            'India': ['GDP_Growth', 'CPI_Inflation', 'Unemployment_Rate', 'Repo_Rate'],\n            'UK': ['GDP_Growth', 'Inflation_Rate', 'Unemployment', 'Bank_Rate'],\n            'Germany': ['GDP_Growth', 'HICP_Inflation', 'Unemployment_Rate', 'ECB_Rate'],\n            'Japan': ['GDP_Growth', 'Core_Inflation', 'Unemployment', 'Policy_Rate']\n        }\n        \n    def load_sample_data(self):\n        \"\"\"\n        Load sample retail data for demonstration and testing.\n        Creates realistic market data with seasonal patterns.\n        \"\"\"\n        try:\n            # Check if sample data file exists\n            if os.path.exists('data/sample_retail_data.csv'):\n                return pd.read_csv('data/sample_retail_data.csv', parse_dates=['Date'])\n            \n            # Generate sample data if file doesn't exist\n            return self._generate_sample_data()\n            \n        except Exception as e:\n            print(f\"Error loading sample data: {e}\")\n            return self._generate_sample_data()\n    \n    def _generate_sample_data(self):\n        \"\"\"Generate realistic sample market data with trends and seasonality\"\"\"\n        \n        # Date range for 2 years of data\n        start_date = datetime(2022, 1, 1)\n        end_date = datetime(2024, 1, 1)\n        date_range = pd.date_range(start_date, end_date, freq='D')\n        \n        n_days = len(date_range)\n        \n        # Base sales trend with growth\n        base_sales = np.linspace(1000, 1500, n_days)\n        \n        # Add seasonality (higher sales in Q4, lower in Q1)\n        seasonal_factor = np.sin(2 * np.pi * np.arange(n_days) / 365.25) * 200 + 100\n        \n        # Add weekly pattern (higher sales on weekends)\n        weekly_factor = np.sin(2 * np.pi * np.arange(n_days) / 7) * 50\n        \n        # Add random noise\n        noise = np.random.normal(0, 50, n_days)\n        \n        # Combine all factors\n        sales = base_sales + seasonal_factor + weekly_factor + noise\n        sales = np.maximum(sales, 0)  # Ensure positive values\n        \n        # Generate related metrics\n        customers = np.random.poisson(sales / 10)  # Customers related to sales\n        avg_order_value = sales / customers\n        avg_order_value = np.where(customers == 0, 0, avg_order_value)\n        \n        # Product categories\n        categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books']\n        category_weights = np.random.dirichlet([1, 1, 1, 1, 1], n_days)\n        \n        # Customer demographics\n        age_groups = ['18-25', '26-35', '36-45', '46-55', '55+']\n        age_weights = np.random.dirichlet([0.2, 0.3, 0.25, 0.15, 0.1], n_days)\n        \n        # Create DataFrame\n        sample_data = pd.DataFrame({\n            'Date': date_range,\n            'Sales': sales.round(2),\n            'Customers': customers,\n            'Average_Order_Value': avg_order_value.round(2),\n            'Electronics_Sales': (sales * category_weights[:, 0]).round(2),\n            'Clothing_Sales': (sales * category_weights[:, 1]).round(2),\n            'Home_Garden_Sales': (sales * category_weights[:, 2]).round(2),\n            'Sports_Sales': (sales * category_weights[:, 3]).round(2),\n            'Books_Sales': (sales * category_weights[:, 4]).round(2),\n            'Age_18_25_Customers': (customers * age_weights[:, 0]).round(0).astype(int),\n            'Age_26_35_Customers': (customers * age_weights[:, 1]).round(0).astype(int),\n            'Age_36_45_Customers': (customers * age_weights[:, 2]).round(0).astype(int),\n            'Age_46_55_Customers': (customers * age_weights[:, 3]).round(0).astype(int),\n            'Age_55_Plus_Customers': (customers * age_weights[:, 4]).round(0).astype(int),\n        })\n        \n        # Add some customer behavior metrics\n        sample_data['Return_Rate'] = np.random.uniform(0.05, 0.15, n_days).round(3)\n        sample_data['Customer_Satisfaction'] = np.random.uniform(3.5, 5.0, n_days).round(2)\n        sample_data['Marketing_Spend'] = (sales * np.random.uniform(0.05, 0.12, n_days)).round(2)\n        \n        # Set date as index\n        sample_data.set_index('Date', inplace=True)\n        \n        return sample_data\n    \n    def get_stock_symbol_format(self, symbol):\n        \"\"\"Convert stock symbol to proper Yahoo Finance format\"\"\"\n        symbol_upper = symbol.upper()\n        if symbol_upper in self.indian_stocks:\n            return self.indian_stocks[symbol_upper]\n        elif '.NS' in symbol or '.BO' in symbol or '.L' in symbol:\n            return symbol\n        else:\n            return symbol\n    \n    def calculate_vcp_pattern(self, df):\n        \"\"\"Calculate Volatility Contraction Pattern (VCP) indicators\"\"\"\n        try:\n            vcp_data = df.copy()\n            \n            # Calculate Average True Range (ATR)\n            high_low = vcp_data['High'] - vcp_data['Low']\n            high_close = np.abs(vcp_data['High'] - vcp_data['Close'].shift())\n            low_close = np.abs(vcp_data['Low'] - vcp_data['Close'].shift())\n            \n            ranges = pd.concat([high_low, high_close, low_close], axis=1)\n            true_range = ranges.max(axis=1)\n            vcp_data['ATR'] = true_range.rolling(window=14).mean()\n            \n            # Calculate volatility contraction\n            vcp_data['Volatility_20'] = vcp_data['Returns'].rolling(window=20).std()\n            vcp_data['Volatility_10'] = vcp_data['Returns'].rolling(window=10).std()\n            vcp_data['VCP_Ratio'] = vcp_data['Volatility_10'] / vcp_data['Volatility_20']\n            \n            # VCP Signal: when current volatility is lower than historical\n            vcp_data['VCP_Signal'] = vcp_data['VCP_Ratio'] < 0.8\n            \n            # Price consolidation check\n            vcp_data['Price_Range_Pct'] = (vcp_data['High'].rolling(20).max() - \n                                          vcp_data['Low'].rolling(20).min()) / vcp_data['Close'] * 100\n            \n            # Tight consolidation when price range is small\n            vcp_data['Tight_Consolidation'] = vcp_data['Price_Range_Pct'] < 15\n            \n            return vcp_data\n            \n        except Exception as e:\n            print(f\"Error calculating VCP: {e}\")\n            return df\n    \n    def detect_trend_signals(self, df):\n        \"\"\"Detect bullish and bearish trend signals\"\"\"\n        try:\n            trend_data = df.copy()\n            \n            # Moving averages for trend detection\n            trend_data['MA_5'] = trend_data['Close'].rolling(window=5).mean()\n            trend_data['MA_10'] = trend_data['Close'].rolling(window=10).mean()\n            trend_data['MA_20'] = trend_data['Close'].rolling(window=20).mean()\n            trend_data['MA_50'] = trend_data['Close'].rolling(window=50).mean()\n            \n            # Bullish signals\n            trend_data['Golden_Cross'] = (trend_data['MA_20'] > trend_data['MA_50']) & \\\n                                       (trend_data['MA_20'].shift() <= trend_data['MA_50'].shift())\n            \n            trend_data['Price_Above_MA20'] = trend_data['Close'] > trend_data['MA_20']\n            trend_data['Volume_Surge'] = trend_data['Volume'] > trend_data['Volume'].rolling(20).mean() * 1.5\n            \n            # Bearish signals\n            trend_data['Death_Cross'] = (trend_data['MA_20'] < trend_data['MA_50']) & \\\n                                      (trend_data['MA_20'].shift() >= trend_data['MA_50'].shift())\n            \n            trend_data['Price_Below_MA20'] = trend_data['Close'] < trend_data['MA_20']\n            \n            # Overall trend classification\n            bullish_conditions = trend_data[['Price_Above_MA20']].sum(axis=1)\n            bearish_conditions = trend_data[['Price_Below_MA20']].sum(axis=1)\n            \n            trend_data['Trend'] = 'Neutral'\n            trend_data.loc[bullish_conditions > bearish_conditions, 'Trend'] = 'Bullish'\n            trend_data.loc[bearish_conditions > bullish_conditions, 'Trend'] = 'Bearish'\n            \n            return trend_data\n            \n        except Exception as e:\n            print(f\"Error detecting trends: {e}\")\n            return df\n    \n    def get_stock_data(self, symbol, period=\"1y\"):\n        \"\"\"\n        Fetch stock market data using Yahoo Finance API.\n        \n        Args:\n            symbol (str): Stock symbol (e.g., 'AAPL', 'GOOGL')\n            period (str): Time period ('1y', '2y', '5y', etc.)\n        \n        Returns:\n            pd.DataFrame: Stock price data with OHLCV\n        \"\"\"\n        try:\n            # Get proper symbol format\n            yahoo_symbol = self.get_stock_symbol_format(symbol)\n            \n            # Check cache first\n            if yahoo_symbol in self.cache:\n                return self.cache[yahoo_symbol]\n            \n            ticker = yf.Ticker(yahoo_symbol)\n            stock_data = ticker.history(period=period)\n            \n            if stock_data.empty:\n                # Try with .NS extension for Indian stocks if not already present\n                if not yahoo_symbol.endswith('.NS') and not yahoo_symbol.endswith('.BO'):\n                    yahoo_symbol = f\"{yahoo_symbol}.NS\"\n                    ticker = yf.Ticker(yahoo_symbol)\n                    stock_data = ticker.history(period=period)\n                \n                if stock_data.empty:\n                    raise ValueError(f\"No data found for symbol: {symbol}\")\n            \n            # Add technical indicators\n            stock_data['Returns'] = stock_data['Close'].pct_change()\n            stock_data['Volatility'] = stock_data['Returns'].rolling(window=20).std()\n            stock_data['MA_20'] = stock_data['Close'].rolling(window=20).mean()\n            stock_data['MA_50'] = stock_data['Close'].rolling(window=50).mean()\n            \n            # Calculate RSI\n            delta = stock_data['Close'].diff()\n            gain = delta.where(delta > 0, 0)\n            loss = -delta.where(delta < 0, 0)\n            avg_gain = gain.rolling(window=14).mean()\n            avg_loss = loss.rolling(window=14).mean()\n            rs = avg_gain / avg_loss\n            stock_data['RSI'] = 100 - (100 / (1 + rs))\n            \n            # Add VCP analysis\n            stock_data = self.calculate_vcp_pattern(stock_data)\n            \n            # Add trend detection\n            stock_data = self.detect_trend_signals(stock_data)\n            \n            # Cache the data\n            self.cache[yahoo_symbol] = stock_data\n            \n            return stock_data\n            \n        except Exception as e:\n            print(f\"Error fetching stock data: {e}\")\n            # Return sample stock data as fallback\n            return self._generate_sample_stock_data(symbol)\n    \n    def _generate_sample_stock_data(self, symbol):\n        \"\"\"Generate sample stock data for demonstration\"\"\"\n        dates = pd.date_range(end=datetime.now(), periods=365, freq='D')\n        \n        # Generate realistic stock price movement\n        initial_price = 100\n        prices = [initial_price]\n        \n        for i in range(1, len(dates)):\n            # Random walk with slight upward bias\n            change = np.random.normal(0.001, 0.02)  # 0.1% average daily return, 2% volatility\n            new_price = prices[-1] * (1 + change)\n            prices.append(max(new_price, 1.0))  # Ensure positive prices\n        \n        # Generate OHLC data\n        close_prices = np.array(prices)\n        high_prices = close_prices * (1 + np.random.uniform(0, 0.03, len(close_prices)))\n        low_prices = close_prices * (1 - np.random.uniform(0, 0.03, len(close_prices)))\n        open_prices = np.roll(close_prices, 1)\n        open_prices[0] = close_prices[0]\n        \n        # Volume\n        volumes = np.random.lognormal(15, 0.5, len(dates)).astype(int)\n        \n        stock_data = pd.DataFrame({\n            'Open': open_prices,\n            'High': high_prices,\n            'Low': low_prices,\n            'Close': close_prices,\n            'Volume': volumes\n        }, index=dates)\n        \n        # Add technical indicators\n        stock_data['Returns'] = stock_data['Close'].pct_change()\n        stock_data['Volatility'] = stock_data['Returns'].rolling(window=20).std()\n        stock_data['MA_20'] = stock_data['Close'].rolling(window=20).mean()\n        stock_data['MA_50'] = stock_data['Close'].rolling(window=50).mean()\n        \n        return stock_data\n    \n    def search_stocks(self, query):\n        \"\"\"Search for stocks based on query\"\"\"\n        query_upper = query.upper()\n        matches = []\n        \n        # Search in Indian stocks\n        for name, symbol in self.indian_stocks.items():\n            if query_upper in name or query_upper in symbol:\n                matches.append({'name': name, 'symbol': symbol, 'market': 'Indian'})\n        \n        # Add common US stocks for search\n        us_stocks = {\n            'APPLE': 'AAPL', 'MICROSOFT': 'MSFT', 'GOOGLE': 'GOOGL', \n            'AMAZON': 'AMZN', 'TESLA': 'TSLA', 'META': 'META',\n            'NVIDIA': 'NVDA', 'NETFLIX': 'NFLX', 'ADOBE': 'ADBE'\n        }\n        \n        for name, symbol in us_stocks.items():\n            if query_upper in name or query_upper in symbol:\n                matches.append({'name': name, 'symbol': symbol, 'market': 'US'})\n        \n        return matches[:10]  # Return top 10 matches\n    \n    def get_economic_indicators(self, country='US'):\n        \"\"\"\n        Fetch economic indicators data.\n        In a real implementation, this would connect to economic data APIs.\n        \"\"\"\n        try:\n            # Generate country-specific economic data\n            dates = pd.date_range(end=datetime.now(), periods=60, freq='M')\n            \n            if country == 'India':\n                economic_data = pd.DataFrame({\n                    'Date': dates,\n                    'GDP_Growth': np.random.normal(6.5, 1.2, len(dates)),\n                    'CPI_Inflation': np.random.normal(5.8, 1.0, len(dates)),\n                    'Unemployment_Rate': np.random.normal(7.2, 1.0, len(dates)),\n                    'Repo_Rate': np.random.normal(6.0, 0.8, len(dates)),\n                    'Industrial_Production': np.random.normal(4.2, 2.0, len(dates)),\n                    'Forex_Reserves': np.random.normal(600, 20, len(dates))\n                })\n            elif country == 'US':\n                economic_data = pd.DataFrame({\n                    'Date': dates,\n                    'GDP_Growth': np.random.normal(2.5, 1.0, len(dates)),\n                    'Inflation': np.random.normal(3.0, 0.8, len(dates)),\n                    'Unemployment': np.random.normal(4.0, 1.0, len(dates)),\n                    'Interest_Rate': np.random.normal(5.0, 0.5, len(dates)),\n                    'Consumer_Confidence': np.random.normal(100, 15, len(dates))\n                })\n            elif country == 'UK':\n                economic_data = pd.DataFrame({\n                    'Date': dates,\n                    'GDP_Growth': np.random.normal(1.8, 1.2, len(dates)),\n                    'Inflation_Rate': np.random.normal(2.1, 0.9, len(dates)),\n                    'Unemployment': np.random.normal(3.8, 0.8, len(dates)),\n                    'Bank_Rate': np.random.normal(5.25, 0.6, len(dates))\n                })\n            else:\n                # Default to US data\n                economic_data = pd.DataFrame({\n                    'Date': dates,\n                    'GDP_Growth': np.random.normal(2.5, 1.0, len(dates)),\n                    'Inflation': np.random.normal(3.0, 0.8, len(dates)),\n                    'Unemployment': np.random.normal(4.0, 1.0, len(dates)),\n                    'Interest_Rate': np.random.normal(5.0, 0.5, len(dates))\n                })\n            \n            economic_data.set_index('Date', inplace=True)\n            return economic_data\n            \n        except Exception as e:\n            print(f\"Error fetching economic data: {e}\")\n            return pd.DataFrame()\n    \n    def collect_social_media_data(self, query, limit=100):\n        \"\"\"\n        Collect social media data for sentiment analysis.\n        This is a placeholder for social media API integration.\n        \n        Args:\n            query (str): Search query\n            limit (int): Number of posts to collect\n        \n        Returns:\n            list: List of text posts\n        \"\"\"\n        # Placeholder implementation\n        # In real implementation, integrate with Twitter API, Reddit API, etc.\n        \n        sample_posts = [\n            f\"Great product! Love the {query} experience!\",\n            f\"Not satisfied with {query}. Poor quality.\",\n            f\"Amazing {query}! Highly recommend to everyone.\",\n            f\"Average {query}, nothing special but works fine.\",\n            f\"Terrible {query} experience. Would not buy again.\",\n            f\"Outstanding {query} quality! Best purchase ever!\",\n            f\"Disappointing {query}. Expected much better.\",\n            f\"Excellent {query} service! Five stars!\",\n            f\"Mediocre {query}. Price is too high for quality.\",\n            f\"Perfect {query}! Exactly what I needed.\"\n        ]\n        \n        # Return random sample of posts\n        import random\n        return random.sample(sample_posts, min(limit, len(sample_posts)))\n    \n    def validate_data_quality(self, df):\n        \"\"\"\n        Validate data quality and return quality metrics.\n        \n        Args:\n            df (pd.DataFrame): Data to validate\n        \n        Returns:\n            dict: Data quality metrics\n        \"\"\"\n        quality_metrics = {\n            'total_rows': len(df),\n            'total_columns': len(df.columns),\n            'missing_values': df.isnull().sum().sum(),\n            'missing_percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,\n            'duplicate_rows': df.duplicated().sum(),\n            'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),\n            'categorical_columns': len(df.select_dtypes(include=['object']).columns),\n            'date_columns': len(df.select_dtypes(include=['datetime64']).columns)\n        }\n        \n        # Check for outliers in numeric columns\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        outlier_counts = {}\n        \n        for col in numeric_cols:\n            Q1 = df[col].quantile(0.25)\n            Q3 = df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n            outlier_counts[col] = outliers\n        \n        quality_metrics['outliers_by_column'] = outlier_counts\n        quality_metrics['total_outliers'] = sum(outlier_counts.values())\n        \n        return quality_metrics\n    \n    def preprocess_raw_data(self, df):\n        \"\"\"\n        Basic preprocessing for raw data.\n        \n        Args:\n            df (pd.DataFrame): Raw data\n        \n        Returns:\n            pd.DataFrame: Preprocessed data\n        \"\"\"\n        processed_df = df.copy()\n        \n        # Handle missing values\n        numeric_cols = processed_df.select_dtypes(include=[np.number]).columns\n        categorical_cols = processed_df.select_dtypes(include=['object']).columns\n        \n        # Fill numeric missing values with median\n        for col in numeric_cols:\n            processed_df[col].fillna(processed_df[col].median(), inplace=True)\n        \n        # Fill categorical missing values with mode\n        for col in categorical_cols:\n            mode_value = processed_df[col].mode().iloc[0] if not processed_df[col].mode().empty else 'Unknown'\n            processed_df[col].fillna(mode_value, inplace=True)\n        \n        # Remove duplicates\n        processed_df.drop_duplicates(inplace=True)\n        \n        # Basic outlier treatment for numeric columns\n        for col in numeric_cols:\n            Q1 = processed_df[col].quantile(0.25)\n            Q3 = processed_df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            \n            # Cap outliers instead of removing them\n            processed_df[col] = processed_df[col].clip(lower=lower_bound, upper=upper_bound)\n        \n        return processed_df\n","size_bytes":21735},"forecasting_models.py":{"content":"import pandas as pd\nimport numpy as np\nfrom prophet import Prophet\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass TimeSeriesForecaster:\n    \"\"\"\n    Time series forecasting module supporting multiple algorithms.\n    Includes Prophet, ARIMA, Linear Regression, and Random Forest models.\n    \"\"\"\n    \n    def __init__(self):\n        self.models = {}\n        self.scalers = {}\n        \n    def prophet_forecast(self, y, periods=30, include_seasonality=True):\n        \"\"\"\n        Perform time series forecasting using Facebook Prophet.\n        \n        Args:\n            y (pd.Series): Time series data with datetime index\n            periods (int): Number of periods to forecast\n            include_seasonality (bool): Include seasonal components\n        \n        Returns:\n            dict: Forecast results with metrics\n        \"\"\"\n        try:\n            # Prepare data for Prophet\n            df_prophet = pd.DataFrame({\n                'ds': y.index,\n                'y': y.values\n            })\n            \n            # Initialize Prophet model\n            prophet_params = {\n                'yearly_seasonality': include_seasonality,\n                'weekly_seasonality': include_seasonality,\n                'daily_seasonality': False,\n                'seasonality_mode': 'multiplicative' if include_seasonality else 'additive'\n            }\n            \n            model = Prophet(**prophet_params)\n            \n            # Add custom seasonalities if requested\n            if include_seasonality:\n                model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n                model.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n            \n            # Fit the model\n            model.fit(df_prophet)\n            \n            # Create future dates\n            future = model.make_future_dataframe(periods=periods)\n            \n            # Generate forecast\n            forecast = model.predict(future)\n            \n            # Extract forecast values\n            forecast_series = pd.Series(\n                forecast['yhat'].iloc[-periods:].values,\n                index=pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), \n                                  periods=periods, freq='D')\n            )\n            \n            # Calculate metrics on historical data\n            historical_forecast = forecast['yhat'].iloc[:-periods]\n            metrics = self._calculate_metrics(y.values, historical_forecast.values)\n            \n            # Store model\n            self.models['prophet'] = model\n            \n            return {\n                'forecast': forecast_series,\n                'model': model,\n                'metrics': metrics,\n                'confidence_intervals': {\n                    'lower': pd.Series(forecast['yhat_lower'].iloc[-periods:].values, \n                                     index=forecast_series.index),\n                    'upper': pd.Series(forecast['yhat_upper'].iloc[-periods:].values, \n                                     index=forecast_series.index)\n                },\n                'components': {\n                    'trend': pd.Series(forecast['trend'].iloc[-periods:].values, \n                                     index=forecast_series.index),\n                    'seasonal': pd.Series(forecast['yearly'].iloc[-periods:].values, \n                                        index=forecast_series.index) if include_seasonality else None\n                }\n            }\n            \n        except Exception as e:\n            print(f\"Error in Prophet forecasting: {e}\")\n            return self._fallback_forecast(y, periods)\n    \n    def arima_forecast(self, y, periods=30, order=None):\n        \"\"\"\n        Perform ARIMA forecasting.\n        \n        Args:\n            y (pd.Series): Time series data\n            periods (int): Number of periods to forecast\n            order (tuple): ARIMA order (p, d, q). If None, auto-determined\n        \n        Returns:\n            dict: Forecast results with metrics\n        \"\"\"\n        try:\n            # Auto-determine ARIMA order if not provided\n            if order is None:\n                order = self._auto_arima_order(y)\n            \n            # Fit ARIMA model\n            model = ARIMA(y, order=order)\n            fitted_model = model.fit()\n            \n            # Generate forecast\n            forecast_result = fitted_model.forecast(steps=periods)\n            \n            # Create forecast series\n            forecast_series = pd.Series(\n                forecast_result,\n                index=pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), \n                                  periods=periods, freq='D')\n            )\n            \n            # Calculate metrics\n            fitted_values = fitted_model.fittedvalues\n            metrics = self._calculate_metrics(y.values[1:], fitted_values.values)  # Skip first value due to differencing\n            \n            # Store model\n            self.models['arima'] = fitted_model\n            \n            return {\n                'forecast': forecast_series,\n                'model': fitted_model,\n                'metrics': metrics,\n                'order': order,\n                'aic': fitted_model.aic,\n                'bic': fitted_model.bic\n            }\n            \n        except Exception as e:\n            print(f\"Error in ARIMA forecasting: {e}\")\n            return self._fallback_forecast(y, periods)\n    \n    def linear_forecast(self, y, periods=30):\n        \"\"\"\n        Simple linear regression forecast.\n        \n        Args:\n            y (pd.Series): Time series data\n            periods (int): Number of periods to forecast\n        \n        Returns:\n            dict: Forecast results with metrics\n        \"\"\"\n        try:\n            # Prepare features (time index as numeric)\n            X = np.arange(len(y)).reshape(-1, 1)\n            \n            # Fit linear regression\n            model = LinearRegression()\n            model.fit(X, y.values)\n            \n            # Generate forecast\n            future_X = np.arange(len(y), len(y) + periods).reshape(-1, 1)\n            forecast_values = model.predict(future_X)\n            \n            # Create forecast series\n            forecast_series = pd.Series(\n                forecast_values,\n                index=pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), \n                                  periods=periods, freq='D')\n            )\n            \n            # Calculate metrics\n            fitted_values = model.predict(X)\n            metrics = self._calculate_metrics(y.values, fitted_values)\n            \n            # Store model\n            self.models['linear'] = model\n            \n            return {\n                'forecast': forecast_series,\n                'model': model,\n                'metrics': metrics,\n                'slope': model.coef_[0],\n                'intercept': model.intercept_,\n                'r_squared': model.score(X, y.values)\n            }\n            \n        except Exception as e:\n            print(f\"Error in linear forecasting: {e}\")\n            return self._fallback_forecast(y, periods)\n    \n    def random_forest_forecast(self, y, periods=30, n_estimators=100, lookback=7):\n        \"\"\"\n        Random Forest regression forecast using lagged features.\n        \n        Args:\n            y (pd.Series): Time series data\n            periods (int): Number of periods to forecast\n            n_estimators (int): Number of trees in forest\n            lookback (int): Number of lagged values to use as features\n        \n        Returns:\n            dict: Forecast results with metrics\n        \"\"\"\n        try:\n            # Create lagged features\n            X, y_train = self._create_lagged_features(y, lookback)\n            \n            # Scale features\n            scaler = StandardScaler()\n            X_scaled = scaler.fit_transform(X)\n            \n            # Fit Random Forest\n            model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n            model.fit(X_scaled, y_train)\n            \n            # Generate forecast iteratively\n            forecast_values = []\n            last_values = y.values[-lookback:].tolist()\n            \n            for _ in range(periods):\n                # Prepare features for next prediction\n                features = np.array(last_values[-lookback:]).reshape(1, -1)\n                features_scaled = scaler.transform(features)\n                \n                # Predict next value\n                next_value = model.predict(features_scaled)[0]\n                forecast_values.append(next_value)\n                \n                # Update last_values for next iteration\n                last_values.append(next_value)\n            \n            # Create forecast series\n            forecast_series = pd.Series(\n                forecast_values,\n                index=pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), \n                                  periods=periods, freq='D')\n            )\n            \n            # Calculate metrics on training data\n            fitted_values = model.predict(X_scaled)\n            metrics = self._calculate_metrics(y_train, fitted_values)\n            \n            # Store model and scaler\n            self.models['random_forest'] = model\n            self.scalers['random_forest'] = scaler\n            \n            return {\n                'forecast': forecast_series,\n                'model': model,\n                'scaler': scaler,\n                'metrics': metrics,\n                'feature_importance': dict(zip([f'lag_{i+1}' for i in range(lookback)], \n                                             model.feature_importances_)),\n                'lookback_period': lookback\n            }\n            \n        except Exception as e:\n            print(f\"Error in Random Forest forecasting: {e}\")\n            return self._fallback_forecast(y, periods)\n    \n    def _create_lagged_features(self, y, lookback):\n        \"\"\"Create lagged features for machine learning models\"\"\"\n        X = []\n        y_train = []\n        \n        for i in range(lookback, len(y)):\n            X.append(y.values[i-lookback:i])\n            y_train.append(y.values[i])\n        \n        return np.array(X), np.array(y_train)\n    \n    def _auto_arima_order(self, y, max_p=3, max_d=2, max_q=3):\n        \"\"\"Automatically determine ARIMA order using AIC\"\"\"\n        best_aic = np.inf\n        best_order = (0, 1, 0)\n        \n        for p in range(max_p + 1):\n            for d in range(max_d + 1):\n                for q in range(max_q + 1):\n                    try:\n                        model = ARIMA(y, order=(p, d, q))\n                        fitted_model = model.fit()\n                        \n                        if fitted_model.aic < best_aic:\n                            best_aic = fitted_model.aic\n                            best_order = (p, d, q)\n                    except:\n                        continue\n        \n        return best_order\n    \n    def _calculate_metrics(self, y_true, y_pred):\n        \"\"\"Calculate forecast accuracy metrics\"\"\"\n        \n        # Handle cases where arrays might have different lengths\n        min_length = min(len(y_true), len(y_pred))\n        y_true = y_true[-min_length:]\n        y_pred = y_pred[-min_length:]\n        \n        try:\n            mae = mean_absolute_error(y_true, y_pred)\n            mse = mean_squared_error(y_true, y_pred)\n            rmse = np.sqrt(mse)\n            mape = mean_absolute_percentage_error(y_true, y_pred)\n            \n            # Additional metrics\n            mean_actual = np.mean(y_true)\n            mean_forecast = np.mean(y_pred)\n            bias = mean_forecast - mean_actual\n            \n            return {\n                'mae': mae,\n                'mse': mse,\n                'rmse': rmse,\n                'mape': mape * 100,  # Convert to percentage\n                'bias': bias,\n                'mean_actual': mean_actual,\n                'mean_forecast': mean_forecast\n            }\n        except:\n            # Return default metrics if calculation fails\n            return {\n                'mae': 0.0,\n                'mse': 0.0,\n                'rmse': 0.0,\n                'mape': 0.0,\n                'bias': 0.0,\n                'mean_actual': np.mean(y_true),\n                'mean_forecast': np.mean(y_pred)\n            }\n    \n    def _fallback_forecast(self, y, periods):\n        \"\"\"Fallback forecast using simple trend extrapolation\"\"\"\n        try:\n            # Calculate simple linear trend\n            x = np.arange(len(y))\n            trend = np.polyfit(x, y.values, 1)\n            \n            # Extrapolate trend\n            future_x = np.arange(len(y), len(y) + periods)\n            forecast_values = np.polyval(trend, future_x)\n            \n            # Create forecast series\n            forecast_series = pd.Series(\n                forecast_values,\n                index=pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), \n                                  periods=periods, freq='D')\n            )\n            \n            # Simple metrics\n            fitted_values = np.polyval(trend, x)\n            metrics = self._calculate_metrics(y.values, fitted_values)\n            \n            return {\n                'forecast': forecast_series,\n                'model': 'fallback_trend',\n                'metrics': metrics,\n                'trend_slope': trend[0],\n                'trend_intercept': trend[1]\n            }\n        except:\n            # Last resort: use mean value\n            mean_value = y.mean()\n            forecast_series = pd.Series(\n                [mean_value] * periods,\n                index=pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), \n                                  periods=periods, freq='D')\n            )\n            \n            return {\n                'forecast': forecast_series,\n                'model': 'fallback_mean',\n                'metrics': {'mae': 0, 'mse': 0, 'rmse': 0, 'mape': 0, 'bias': 0,\n                           'mean_actual': mean_value, 'mean_forecast': mean_value}\n            }\n    \n    def decompose_time_series(self, y, model='additive', period=None):\n        \"\"\"\n        Decompose time series into trend, seasonal, and residual components.\n        \n        Args:\n            y (pd.Series): Time series data\n            model (str): 'additive' or 'multiplicative'\n            period (int): Seasonal period\n        \n        Returns:\n            dict: Decomposition components\n        \"\"\"\n        try:\n            if period is None:\n                period = min(len(y) // 2, 365)  # Default to yearly or half the data length\n            \n            decomposition = seasonal_decompose(y, model=model, period=period)\n            \n            return {\n                'trend': decomposition.trend,\n                'seasonal': decomposition.seasonal,\n                'residual': decomposition.resid,\n                'observed': decomposition.observed\n            }\n        except Exception as e:\n            print(f\"Error in time series decomposition: {e}\")\n            return None\n    \n    def generate_insights(self, historical_data, forecast_data):\n        \"\"\"\n        Generate business insights from forecasting results.\n        \n        Args:\n            historical_data (pd.Series): Historical time series\n            forecast_data (pd.Series): Forecast time series\n        \n        Returns:\n            list: List of insight strings\n        \"\"\"\n        insights = []\n        \n        try:\n            # Trend analysis\n            historical_mean = historical_data.mean()\n            forecast_mean = forecast_data.mean()\n            \n            if forecast_mean > historical_mean * 1.05:\n                insights.append(f\"ðŸ“ˆ Forecast shows positive growth trend: {((forecast_mean / historical_mean - 1) * 100):.1f}% increase expected\")\n            elif forecast_mean < historical_mean * 0.95:\n                insights.append(f\"ðŸ“‰ Forecast indicates declining trend: {((1 - forecast_mean / historical_mean) * 100):.1f}% decrease expected\")\n            else:\n                insights.append(\"ðŸ“Š Forecast suggests stable market conditions with minimal change\")\n            \n            # Volatility analysis\n            historical_volatility = historical_data.std()\n            forecast_volatility = forecast_data.std()\n            \n            if forecast_volatility > historical_volatility * 1.2:\n                insights.append(\"âš ï¸ Higher volatility expected in forecast period - consider risk management strategies\")\n            elif forecast_volatility < historical_volatility * 0.8:\n                insights.append(\"âœ… Lower volatility expected - more predictable market conditions ahead\")\n            \n            # Peak and trough analysis\n            forecast_max = forecast_data.max()\n            forecast_min = forecast_data.min()\n            historical_max = historical_data.max()\n            \n            if forecast_max > historical_max:\n                insights.append(f\"ðŸŽ¯ New peak expected: forecast maximum ({forecast_max:.2f}) exceeds historical high ({historical_max:.2f})\")\n            \n            # Seasonal patterns\n            if len(forecast_data) >= 30:\n                monthly_avg = forecast_data.groupby(forecast_data.index.month).mean()\n                best_month = monthly_avg.idxmax()\n                worst_month = monthly_avg.idxmin()\n                \n                month_names = {1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n                              7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n                \n                insights.append(f\"ðŸ“… Best forecasted month: {month_names.get(best_month, best_month)} | Worst: {month_names.get(worst_month, worst_month)}\")\n            \n            # Business recommendations\n            trend_slope = (forecast_data.iloc[-1] - forecast_data.iloc[0]) / len(forecast_data)\n            \n            if trend_slope > 0:\n                insights.append(\"ðŸ’¼ Recommendation: Consider increasing inventory and marketing spend to capture growth opportunity\")\n            else:\n                insights.append(\"ðŸ’¼ Recommendation: Focus on cost optimization and customer retention strategies\")\n            \n        except Exception as e:\n            insights.append(f\"âš ï¸ Error generating detailed insights: {str(e)}\")\n        \n        return insights\n","size_bytes":18806},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"streamlit>=1.49.1\",\n]\n\n[[tool.uv.index]]\nexplicit = true\nname = \"pytorch-cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\n\n[tool.uv.sources]\nAA-module = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nABlooper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nAnalysisG = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nAutoRAG = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nBERTeam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nBxTorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nByaldi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCALM-Pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCOPEX-high-rate-compression-quality-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCityLearn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCoCa-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCoLT5-attention = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nComfyUI-EasyNodes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCrawl4AI = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDALL-E = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDI-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDatasetRising = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDeepCache = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDeepMatter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDraugr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nESRNN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nEn-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nExpoSeq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nFLAML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nFSRS-Optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGANDLF = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGQLAlchemy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGhostScan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGraKeL = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nHEBO = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nIOPaint = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nISLP = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nInvokeAI = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nJAEN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nKapoorLabs-Lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nLightAutoML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nLingerGRN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nMMEdu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nMRzeroCore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nModeva = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNeuralFoil = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNiMARE = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNinjaTools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nOpenHosta = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nOpenNMT-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPOT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPVNet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPaLM-rlhf-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPepperPepper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPiML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPoutyne = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nQNCP = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRAGatouille = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRareGO = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRealtimeSTT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRelevanceAI-Workflows-Core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nResemblyzer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nScandEval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nSimba-UW-tf-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nSwissArmyTransformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTPOT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTTS = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTorchCRF = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTotalSegmentator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nUtilsRL = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nWhisperSpeech = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nXAISuite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\na-unet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\na5dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccelerate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccelerated-scan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccern-xyme = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nachatbot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nacids-rave = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nactorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nacvl-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadabelief-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadam-atan2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadan-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadapters = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadmin-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadtoolbox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadversarial-robustness-toolbox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naeiou = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naeon = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nafricanwhisper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nag-llama-api = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nagentdojo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nagilerl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-edge-torch-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-parrot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-transform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-olmo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-olmo-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-tango = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naicmder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naider-chat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naider-chat-x = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naif360 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naihwkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naimodelshare = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairtestProject = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairunner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naisak = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naislib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naisquared = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naistore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naithree = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nakasha-terminal = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalibi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalibi-detect = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalignn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nall-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp-pvt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallophant = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallosaurus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naloy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalpaca-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphafold2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphafold3-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphamed-federated = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphawave = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\namazon-braket-pennylane-plugin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\namazon-photos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanemoi-graphs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanemoi-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanomalib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\napache-beam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\napache-tvm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naperturedb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naphrodite-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naqlm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narcAGI2024 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narchisound = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nargbind = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narize = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narm-pytorch-utilities = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narray-api-compat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nassert-llm-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nasteroid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nasteroid-filterbanks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nastra-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nastrovision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\natomate2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nattacut = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-encoders-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-separator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudiocraft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudiolm-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauralis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauraloss = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauto-gptq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautoawq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautoawq-kernels = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.multimodal\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.tabular\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.timeseries\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautotrain-advanced = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\navdeepfake1m = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naws-fortuna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nax-platform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-automl-dnn-vision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-contrib-automl-dnn-forecasting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-evaluate-mlflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-train-automl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nb2bTools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbackpack-for-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbalrog-nle = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatch-face = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchalign = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchgeneratorsv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchtensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbbrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbenchpots = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbent = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbert-score = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbertopic = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbertviz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbestOf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbetty-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbig-sleep = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-core-cpp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-core-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-nano = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"bioimageio.core\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbitfount = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbitsandbytes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbittensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbittensor-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblackboxopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblanc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblindai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbm25-pt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nboltz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbotorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nboxmot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrainchain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbraindecode = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrevitas = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbriton = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrowsergym-visualwebarena = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbuzz-captions = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbyotrack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbyzerllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nc4v-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncalflops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncame-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncamel-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncamel-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncannai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncaptum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncarte-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncarvekit-colab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncatalyst = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausalml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausalnex = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncbrkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncca-zoo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncdp-backend = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellacdc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellfinder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellpose = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellxgene-census = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchattts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchemprop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchgnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchitra = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncircuitsvis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncjm-yolox-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclarinpl-embeddings = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclass-resolver = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassifier-free-guidance-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassiq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassy-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclean-fid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncleanvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-anytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-benchmark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-by-openai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-interrogator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-retrieval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncltk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclusterops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncnocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncnstd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoba = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncofi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncolbert-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncolpali-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-ray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-ray-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-train = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-train-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressed-tensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressed-tensors-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconcrete-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconfit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontextualSpellCheck = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontinual-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontrolnet-aux = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconvokit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoola = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoqui-tts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoqui-tts-trainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncraft-text-detector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncreme = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncrocodile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncrowd-kit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncryoSPHERE = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncsle-common = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncsle-system-identification = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nctgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncurated-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncut-cross-entropy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncvat-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncybertask = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nd3rlpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndalle-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndalle2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndanila-lib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndanling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndarts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndarwin-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndata-gradients = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatachain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndataclass-array = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndataeval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatarobot-drum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatarobotx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatumaro = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndctorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeep-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepchecks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepchem = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepctr-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepecho = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepepochs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepforest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeeplabcut = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepmd-kit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepmultilingualpunctuation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeeprobust = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepsparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepsparse-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepspeed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndenoising-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndescript-audio-codec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndescript-audiotools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndetecto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndetoxify = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndgenerate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndghs-imgutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndgl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndialogy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndice-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffgram = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffusers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndistilabel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndistrifuser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndnikit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocarray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndoclayout-yolo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocling-ibm-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocquery = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndomino-code-assist = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndreamsim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndropblock = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndruida = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndvclive = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne2-tts-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne2cnn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne3nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neasyocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nebtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\necallisto-ng = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nedsnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neffdet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neinx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neir-dl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neis1600 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neland = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nema-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nembedchain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nenformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nentmax = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nesm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nespaloma-charge = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nespnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\netils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\netna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevadb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevalscope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevaluate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nexllamav2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nextractable = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nface-alignment = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfacenet-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfacexlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfair-esm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq2n = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfaker-file = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfarm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfast-bert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfast-pytorch-kmeans = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastcore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastestimator-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfasttreeshap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfedml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfelupe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfemr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfft-conv-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfickling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfireworks-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflair = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflashrag-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflax = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflexgen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflgo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflopth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflowcept = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflytekitplugins-kfpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflytekitplugins-onnxpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfmbench = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfocal-frequency-loss = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfoldedtensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfractal-tasks-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfreegenius = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfreqtrade = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfschat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunasr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunctorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunlbm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunsor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngalore-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngarak = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngarf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngateloop-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngeffnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngenutility = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngfpgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngigagan-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngin-config = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nglasflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngliner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngluonts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngmft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngoogle-cloud-aiplatform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpforecaster = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpt3discord = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngrad-cam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngraph-weather = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngraphistry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngravitorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngretel-synthetics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngsplat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nguardrails-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nguidance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngymnasium = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhanlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhappytransformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhbutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nheavyball = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhezar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhf-deepali = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhf-doc-builder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhigher = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhjxdl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhkkang-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhordelib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhpsv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhuggingface-hub = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhummingbird-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhvae-backbone = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhya = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhypothesis-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-metrics-plugin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-watson-machine-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-watsonx-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nicetk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nicevision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niden = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nidvpackage = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niglovikov-helper-functions = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimagededup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimagen-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimaginAIry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimg2vec-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nincendio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninference-gpu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfinity-emb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfo-nce-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfoapps-mlops-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-dolomite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-sdg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-training = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninvisible-watermark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niobm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nipex-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niree-turbine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-azure-openai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-torchvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-training = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nitem-matching = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nivadomed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njaqpotpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njina = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njudo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njunky = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk-diffusion = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk1lib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkappadata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkappamodules = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkarbonn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkats = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkbnf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkedro-datasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkeybert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkeytotext = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkhoj = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkiui = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkonfuzio-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkornia = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkornia-moons = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkraken = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkwarray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkwimage = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlabml-nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlagent = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlaion-clap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlama-cleaner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlancedb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangcheck = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangroid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangtest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlayoutparser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nldp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleafmap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleap-ie = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleibniz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleptonai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nletmedoit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlhotse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlib310 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibpecos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibrec-auto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibretranslate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliger-kernel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliger-kernel-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-bolts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-fabric = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-habana = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-lite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightrag = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightweight-gan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightwood = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinear-attention-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinear-operator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliom-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlit-nlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitelama = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitgpt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-adapter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-instructor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-llms-huggingface = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-postprocessor-colbert-rerank = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-blender = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-foundry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-guard = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-rs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmcompressor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmlingua = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmvm-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlm-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlmdeploy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlmms-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlocal-attention = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlovely-tensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlpips = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlycoris-lora = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmace-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagic-pdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagicsoup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagvit2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmaite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanga-ocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanifest-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanipulation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmarker-pdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmatgl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmed-imagetools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedaka = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedcat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedmnist = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmegablocks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmegatron-energon = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmemos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmeshgpt-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmetatensor-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmflux = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmia-vgg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmiditok = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nminari = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nminicons = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nml2rt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlagents = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlbench-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlcroissant = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlpfile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlx-whisper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmaction2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmengine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmengine-lite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmpose = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmsegmentation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodeci-mdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodel2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodelscope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodelspec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonai-weekly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonotonic-alignment-search = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonty = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmosaicml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmosaicml-streaming = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmoshi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmteb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmtmtrain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmulti-quantization = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmyhand = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnGPT-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnaeural-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnapari = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnapatrackmater = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnara-wpe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnatten = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnbeats-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnebulae = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnemo-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneptune = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneptune-client = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnerfacc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnerfstudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnessai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnetcal = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneural-rag = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralforecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralnets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralprophet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuspell = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnevergrad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnexfort = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnimblephysics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnirtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnkululeko = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnlptooltest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnAudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnodely = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnsight = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnunetv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnoisereduce = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnonebot-plugin-nailongremove = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnowcasting-dataloader = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnowcasting-forecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnshtrainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnuwa-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnvflare = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnvidia-modelopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nocf-datapipes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nocnn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nogb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nohmeow-blurr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nolive-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nomlt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nommlx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonediff = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonediffx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonnx2pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonnx2torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopacus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-clip-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-flamingo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-interpreter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenbb-terminal-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenmim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenunmix = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-tokenizers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-xai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenwakeword = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopt-einsum-fx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-habana = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-intel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-neuron = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-quanto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptree = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna-dashboard = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna-integration = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noracle-ads = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\norbit-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\notx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutetts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutlines = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutlines-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npaddlenlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npai-easycv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npandasai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npanns-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npatchwork-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npeft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npegasuspy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npelutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npenn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nperforatedai-freemium = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nperformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npetastorm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npfio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npgmpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nphenolrs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nphobos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npi-zero-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npinecone-text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npiq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npix2tex = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npix2text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npnnx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npolicyengine-us-data = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npolyfuzz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npomegranate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npositional-encodings = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nprefigure = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nproduct-key-memory = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nptflops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nptwt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npulser-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npunctuators = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npy2ls = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyabsa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"pyannote.audio\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyawd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyclarity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npycox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyfemtet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyg-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npygrinder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyhealth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyhf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyiqa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npykeen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npykeops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npylance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npylineaGT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npymanopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npymde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npypots = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyqlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyqtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyro-ppl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npysentimiento = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyserini = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npysr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npythainlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npython-doctr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-fid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-forecasting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-ignite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-kinematics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-lightning-bolts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-metric-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-model-summary = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-msssim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-pfn-extras = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-pretrained-bert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-ranger = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-seed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-tabnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-tabular = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-toolbelt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-transformers-pvt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-triton-rocm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-warmup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-wavelets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch_optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch_revgrad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorchcv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorchltr2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyvene = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyvespa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqianfan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqibo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqiskit-machine-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nquanto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nquick-anomaly-detector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision-pytorch-backend = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision-pytorch-learner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nray-lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrclip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrealesrgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrecbole = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrecommenders = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nredcat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nreformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nregex-sampler = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nreplay-rec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrerankers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresearch-framework = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresemble-enhance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresnest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrf-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrf-groundingdino = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrfconv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrich-logger = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nring-attention-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrltrade-test = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrotary-embedding-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrsp-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrust-circuit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns2fft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns3prl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns3torchconnector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsaferx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsafetensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsagemaker-huggingface-inference-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsagemaker-ssh-helper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsalesforce-lavis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsalesforce-merlion = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsamv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscib-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscvi-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsdmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsecretflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegment-anything-hq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegment-anything-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegmentation-models-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nself-rewarding-lm-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsemantic-kernel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsemantic-router = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsenselab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsent2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsentence-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsequence-model-train = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nserotiny = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsevenn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsglang = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nshap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilero-api-server = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilero-vad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilicondiff-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsimclr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsimple-lama-inpainting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsinabs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsixdrepnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskforecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsktime = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsktmls = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nslangtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmartnoise-synth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmashed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmplx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmqtk-descriptors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmqtk-detection = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnntorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnorkel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnowflake-ml-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nso-vits-svc-fork = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsonusai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsony-custom-layers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsotopia = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-curated-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-experimental = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-huggingface-pipelines = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspan-marker = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspandrel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspandrel-extra-arches = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsparrow-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspatialdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspeechbrain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspeechtokenizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspikeinterface = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspikingjelly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotiflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotpython = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotriver = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsquirrel-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-baselines3 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-diffusion-sdkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-ts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanford-stk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanfordnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanza = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstartorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstreamtasks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstruct-eqtable = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstylegan2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsupar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuper-gradients = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuper-image = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuperlinked = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsupervisely = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsurya-ocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsvdiff-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarm-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarmauri = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarms-memory = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswebench = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsyft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsympytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsyne-tune = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsynthcity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nt5 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntab-transformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntabpfn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaming-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaming-transformers-rom1504 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaskwiz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntbparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntecton = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensor-parallel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorcircuit-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensordict = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensordict-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorrt-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntexify = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntext2text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntextattack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntfkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthepipe-api = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthinc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthingsvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthirdai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntianshou = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntidy3d = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntimesfm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntimm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntipo-kgen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntmnt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntoad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntomesd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntop2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-audiomentations = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-dct = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-delaunay = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-directml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ema = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-encoding = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-fidelity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-geometric = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-geopooling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-harmonics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-kmeans = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-lr-finder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-max-mem = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-optimi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ort = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-pitch-shift = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ppr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-pruning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-snippets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-stoi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-struct = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-tensorrt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchani = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchattacks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchaudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchbiggraph = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcfm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcrepe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdatasets-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdiffeq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdyn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchestra = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorcheval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorcheval-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchextractor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfcpe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfun = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfunc-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchgeo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchgeometry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchjpeg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchlayers-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmeta = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmocks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpippy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchprofile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchquantlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec-nightly-cpu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrl-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchscale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchserve = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchserve-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsnapshot-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchstain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsummaryX = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtext = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtnt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtnt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtyping = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchutil = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchvinecopulib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchviz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchx-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchxrayvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntotalspineseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntracebloc-package-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntrainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-lens = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-smaller-training-vocab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformers-domain-adaptation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransfusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransparent-background = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntreescope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntrolo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntsai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntslearn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nttspod = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntxtai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntyro = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nu8darts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nuhg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nuitestrunner-syberos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultimate-rvc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultralytics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultralytics-thop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunav = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunbabel-comet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunderthesea = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunfoldNd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunimernet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunitorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunitxt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunsloth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunsloth-zoo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunstructured = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunstructured-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nutilsd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nv-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvIQA = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvectice = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvector-quantize-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvectorhub-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nversatile-audio-upscaler = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvertexai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvesin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvgg-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvideo-representations-extractor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nviser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvision-datasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvisionmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvisu3d = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvit-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nviturka-nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvllm-flash-attn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvocos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvollseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwavmark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwdoc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisper-live = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisper-timestamped = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisperx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwilds = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwordllama = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nworker-automate-hub = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwxbtool = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nx-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nx-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxaitk_saliency = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxgrammar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxinference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxtts-api-server = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolo-poser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolov5 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolov7-package = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyta-general-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzensvi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzetascale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzuko = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n","size_bytes":90549},"replit.md":{"content":"# AI Market Trend Analysis Platform\n\n## Overview\n\nThis is a comprehensive AI-powered market trend analysis platform built with Streamlit. The application provides end-to-end capabilities for data collection, time series forecasting, customer segmentation, sentiment analysis, and interactive visualization. It's designed to help businesses analyze market trends, predict future patterns, and derive actionable insights from various data sources including financial markets, customer behavior, and social sentiment.\n\nThe platform integrates multiple machine learning approaches including Prophet for time series forecasting, clustering algorithms for customer segmentation, transformer-based models for sentiment analysis, and advanced visualization capabilities for presenting insights.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Framework**: Streamlit web application with custom CSS styling\n- **Layout**: Multi-column responsive design with sidebar navigation\n- **State Management**: Streamlit session state for maintaining user interactions and loaded data\n- **Visualization**: Plotly for interactive charts and graphs with real-time updates\n\n### Backend Architecture\n- **Modular Design**: Clean separation of concerns with dedicated modules for each functional area\n- **Data Processing Pipeline**: Sequential processing from raw data through preprocessing, analysis, and visualization\n- **Model Management**: Centralized model initialization and caching for performance optimization\n- **Error Handling**: Comprehensive exception handling with fallback mechanisms\n\n### Machine Learning Components\n- **Time Series Forecasting**: Facebook Prophet as primary model with ARIMA, Random Forest, and Linear Regression as alternatives\n- **Customer Segmentation**: K-means clustering with RFM analysis, DBSCAN, and Agglomerative clustering options\n- **Sentiment Analysis**: Transformer-based models (RoBERTa, DistilBERT) with TextBlob fallback\n- **Data Preprocessing**: Automated feature engineering, scaling, encoding, and missing value imputation\n\n### Data Storage Architecture\n- **File-based Storage**: CSV files for sample data with automatic generation if files don't exist\n- **In-memory Processing**: Pandas DataFrames for data manipulation and analysis\n- **Caching Strategy**: Streamlit resource caching for model loading and expensive computations\n\n### Visualization System\n- **Interactive Charts**: Plotly-based charts with hover information, zooming, and filtering\n- **Dashboard Layout**: Multi-panel dashboard with trend charts, cluster visualizations, and sentiment metrics\n- **Customizable Views**: Dynamic chart generation based on user selections and data characteristics\n\n## External Dependencies\n\n### Core ML Libraries\n- **transformers**: Hugging Face transformers for pre-trained NLP models\n- **prophet**: Facebook Prophet for time series forecasting\n- **scikit-learn**: Machine learning algorithms for clustering and preprocessing\n- **statsmodels**: Statistical models including ARIMA for time series analysis\n\n### Data Processing\n- **pandas**: Data manipulation and analysis\n- **numpy**: Numerical computing and array operations\n- **yfinance**: Yahoo Finance API for stock market data retrieval\n\n### Visualization\n- **plotly**: Interactive plotting and dashboard creation\n- **streamlit**: Web application framework and UI components\n- **seaborn/matplotlib**: Additional plotting capabilities for statistical visualizations\n\n### Text Processing\n- **textblob**: Natural language processing and sentiment analysis fallback\n- **re**: Regular expressions for text preprocessing\n\n### Additional Integrations\n- **requests**: HTTP library for API calls and data fetching\n- **warnings**: Python warnings management for cleaner output\n- **os**: Environment variable management for API keys and configuration\n\nThe system is designed to be extensible with additional data sources, models, and visualization types through its modular architecture.","size_bytes":4040},"sentiment_analyzer.py":{"content":"import pandas as pd\nimport numpy as np\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nimport re\nimport time\nfrom textblob import TextBlob\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass MarketSentimentAnalyzer:\n    \"\"\"\n    Market sentiment analysis module using transformer-based models.\n    Supports single text analysis, batch processing, and market-specific sentiment insights.\n    \"\"\"\n    \n    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n        \"\"\"\n        Initialize the sentiment analyzer with a pre-trained model.\n        \n        Args:\n            model_name (str): HuggingFace model name for sentiment analysis\n        \"\"\"\n        try:\n            # Load pre-trained sentiment analysis model\n            self.analyzer = pipeline(\n                \"sentiment-analysis\", \n                model=model_name,\n                tokenizer=model_name,\n                return_all_scores=True\n            )\n            self.model_name = model_name\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            \n        except Exception as e:\n            print(f\"Error loading primary model {model_name}: {e}\")\n            # Fallback to DistilBERT\n            try:\n                self.analyzer = pipeline(\n                    \"sentiment-analysis\",\n                    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n                    return_all_scores=True\n                )\n                self.model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n                self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n            except Exception as fallback_error:\n                print(f\"Error loading fallback model: {fallback_error}\")\n                self.analyzer = None\n                self.model_name = \"textblob_fallback\"\n                self.tokenizer = None\n    \n    def analyze_text(self, text):\n        \"\"\"\n        Analyze sentiment of a single text.\n        \n        Args:\n            text (str): Input text for sentiment analysis\n        \n        Returns:\n            dict: Sentiment analysis results\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            # Preprocess text\n            cleaned_text = self._preprocess_text(text)\n            \n            if self.analyzer:\n                # Use transformer model\n                results = self.analyzer(cleaned_text)\n                \n                # Extract sentiment and confidence\n                if isinstance(results[0], list):\n                    # Multiple scores returned\n                    scores = {result['label']: result['score'] for result in results[0]}\n                    best_sentiment = max(scores.keys(), key=lambda k: scores[k])\n                    confidence = scores[best_sentiment]\n                else:\n                    # Single result\n                    best_sentiment = results[0]['label']\n                    confidence = results[0]['score']\n                    scores = {best_sentiment: confidence}\n                \n                # Normalize sentiment labels\n                sentiment = self._normalize_sentiment_label(best_sentiment)\n                \n            else:\n                # Fallback to TextBlob\n                blob = TextBlob(cleaned_text)\n                polarity = blob.sentiment.polarity\n                \n                if polarity > 0.1:\n                    sentiment = \"POSITIVE\"\n                    confidence = (polarity + 1) / 2\n                elif polarity < -0.1:\n                    sentiment = \"NEGATIVE\"\n                    confidence = (1 - polarity) / 2\n                else:\n                    sentiment = \"NEUTRAL\"\n                    confidence = 1 - abs(polarity)\n                \n                scores = {sentiment: confidence}\n            \n            processing_time = time.time() - start_time\n            \n            return {\n                'text': text,\n                'cleaned_text': cleaned_text,\n                'sentiment': sentiment,\n                'confidence': confidence,\n                'scores': scores,\n                'processing_time': processing_time,\n                'model_used': self.model_name\n            }\n            \n        except Exception as e:\n            print(f\"Error analyzing text: {e}\")\n            return {\n                'text': text,\n                'sentiment': 'NEUTRAL',\n                'confidence': 0.5,\n                'scores': {'NEUTRAL': 0.5},\n                'processing_time': time.time() - start_time,\n                'error': str(e)\n            }\n    \n    def analyze_batch(self, texts, batch_size=32):\n        \"\"\"\n        Analyze sentiment of multiple texts in batches.\n        \n        Args:\n            texts (list): List of texts to analyze\n            batch_size (int): Number of texts to process in each batch\n        \n        Returns:\n            list: List of sentiment analysis results\n        \"\"\"\n        try:\n            results = []\n            \n            # Process in batches for efficiency\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i:i + batch_size]\n                \n                if self.analyzer:\n                    # Use transformer model for batch processing\n                    batch_results = []\n                    for text in batch:\n                        result = self.analyze_text(text)\n                        batch_results.append(result)\n                    \n                    results.extend(batch_results)\n                else:\n                    # Fallback batch processing\n                    for text in batch:\n                        result = self.analyze_text(text)\n                        results.append(result)\n            \n            return results\n            \n        except Exception as e:\n            print(f\"Error in batch analysis: {e}\")\n            # Return individual analysis for each text\n            return [self.analyze_text(text) for text in texts]\n    \n    def analyze_market_sentiment(self, texts, market_context=None):\n        \"\"\"\n        Analyze market sentiment with domain-specific adjustments.\n        \n        Args:\n            texts (list): Market-related texts\n            market_context (str): Market context ('stock', 'crypto', 'retail', etc.)\n        \n        Returns:\n            dict: Market sentiment analysis results\n        \"\"\"\n        try:\n            # Analyze individual texts\n            individual_results = self.analyze_batch(texts)\n            \n            # Aggregate results\n            sentiments = [result['sentiment'] for result in individual_results]\n            confidences = [result['confidence'] for result in individual_results]\n            \n            # Calculate aggregate metrics\n            sentiment_counts = pd.Series(sentiments).value_counts()\n            avg_confidence = np.mean(confidences)\n            \n            # Determine overall market sentiment\n            positive_ratio = sentiment_counts.get('POSITIVE', 0) / len(sentiments) if len(sentiments) > 0 else 0\n            negative_ratio = sentiment_counts.get('NEGATIVE', 0) / len(sentiments) if len(sentiments) > 0 else 0\n            neutral_ratio = sentiment_counts.get('NEUTRAL', 0) / len(sentiments) if len(sentiments) > 0 else 0\n            \n            if positive_ratio > 0.5:\n                overall_sentiment = \"BULLISH\"\n            elif negative_ratio > 0.5:\n                overall_sentiment = \"BEARISH\"\n            else:\n                overall_sentiment = \"MIXED\"\n            \n            # Market-specific insights\n            insights = self._generate_market_insights(\n                sentiment_counts, avg_confidence, market_context\n            )\n            \n            return {\n                'individual_results': individual_results,\n                'overall_sentiment': overall_sentiment,\n                'sentiment_distribution': {\n                    'positive': positive_ratio,\n                    'negative': negative_ratio,\n                    'neutral': neutral_ratio\n                },\n                'average_confidence': avg_confidence,\n                'total_texts_analyzed': len(texts),\n                'insights': insights,\n                'market_context': market_context\n            }\n            \n        except Exception as e:\n            print(f\"Error in market sentiment analysis: {e}\")\n            return {\n                'overall_sentiment': 'MIXED',\n                'sentiment_distribution': {'positive': 0.33, 'negative': 0.33, 'neutral': 0.34},\n                'average_confidence': 0.5,\n                'error': str(e)\n            }\n    \n    def _preprocess_text(self, text):\n        \"\"\"\n        Preprocess text for better sentiment analysis.\n        \n        Args:\n            text (str): Raw text\n        \n        Returns:\n            str: Preprocessed text\n        \"\"\"\n        # Convert to string if not already\n        text = str(text)\n        \n        # Remove URLs\n        text = re.sub(r'http\\S+|www.\\S+', '', text)\n        \n        # Remove mentions and hashtags (keep the text)\n        text = re.sub(r'@\\w+', '', text)\n        text = re.sub(r'#(\\w+)', r'\\1', text)\n        \n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        \n        # Remove leading/trailing whitespace\n        text = text.strip()\n        \n        # Limit text length for model compatibility\n        if self.tokenizer:\n            # Truncate to model's max length\n            max_length = self.tokenizer.model_max_length\n            tokens = self.tokenizer.tokenize(text)\n            if len(tokens) > max_length - 2:  # Account for special tokens\n                tokens = tokens[:max_length - 2]\n                text = self.tokenizer.convert_tokens_to_string(tokens)\n        else:\n            # Fallback truncation\n            text = text[:512]\n        \n        return text\n    \n    def _normalize_sentiment_label(self, label):\n        \"\"\"Normalize different model sentiment labels to standard format\"\"\"\n        label_upper = label.upper()\n        \n        # Handle different model label formats\n        if label_upper in ['POSITIVE', 'POS', 'LABEL_2']:\n            return 'POSITIVE'\n        elif label_upper in ['NEGATIVE', 'NEG', 'LABEL_0']:\n            return 'NEGATIVE'\n        elif label_upper in ['NEUTRAL', 'NEU', 'LABEL_1']:\n            return 'NEUTRAL'\n        else:\n            return label_upper\n    \n    def _generate_market_insights(self, sentiment_counts, avg_confidence, market_context):\n        \"\"\"Generate market-specific insights from sentiment analysis\"\"\"\n        insights = []\n        \n        try:\n            total_texts = sentiment_counts.sum()\n            \n            # Overall sentiment insights\n            if sentiment_counts.get('POSITIVE', 0) > sentiment_counts.get('NEGATIVE', 0) * 2:\n                insights.append(\"ðŸš€ Strong positive market sentiment detected - potential buying opportunity\")\n            elif sentiment_counts.get('NEGATIVE', 0) > sentiment_counts.get('POSITIVE', 0) * 2:\n                insights.append(\"ðŸ“‰ Strong negative sentiment - market may be oversold\")\n            else:\n                insights.append(\"âš–ï¸ Mixed market sentiment - exercise caution in trading decisions\")\n            \n            # Confidence insights\n            if avg_confidence > 0.8:\n                insights.append(f\"ðŸŽ¯ High confidence in sentiment analysis ({avg_confidence:.1%}) - reliable signals\")\n            elif avg_confidence < 0.6:\n                insights.append(f\"âš ï¸ Lower confidence in analysis ({avg_confidence:.1%}) - consider additional data sources\")\n            \n            # Volume insights\n            if total_texts > 100:\n                insights.append(f\"ðŸ“Š High discussion volume ({total_texts} texts) - strong market interest\")\n            elif total_texts < 20:\n                insights.append(f\"ðŸ“Š Low discussion volume ({total_texts} texts) - limited market sentiment data\")\n            \n            # Context-specific insights\n            if market_context:\n                if market_context.lower() == 'stock':\n                    insights.append(\"ðŸ“ˆ Consider correlation with trading volume and price movements\")\n                elif market_context.lower() == 'crypto':\n                    insights.append(\"â‚¿ Crypto sentiment often leads price movements - monitor closely\")\n                elif market_context.lower() == 'retail':\n                    insights.append(\"ðŸ›’ Customer sentiment directly impacts purchasing decisions\")\n            \n            return insights\n            \n        except Exception as e:\n            return [f\"âš ï¸ Error generating insights: {str(e)}\"]\n    \n    def explain_sentiment(self, text, analysis_result):\n        \"\"\"\n        Generate an explanation for the sentiment analysis result.\n        \n        Args:\n            text (str): Original text\n            analysis_result (dict): Sentiment analysis result\n        \n        Returns:\n            str: Human-readable explanation\n        \"\"\"\n        try:\n            sentiment = analysis_result['sentiment']\n            confidence = analysis_result['confidence']\n            \n            # Base explanation\n            explanation = f\"The AI model classified this text as **{sentiment}** with {confidence:.1%} confidence. \"\n            \n            # Confidence level explanation\n            if confidence > 0.9:\n                explanation += \"This is a very high confidence prediction, indicating clear sentiment indicators in the text. \"\n            elif confidence > 0.7:\n                explanation += \"This is a high confidence prediction with strong sentiment signals. \"\n            elif confidence > 0.5:\n                explanation += \"This is a moderate confidence prediction - the sentiment is somewhat ambiguous. \"\n            else:\n                explanation += \"This is a low confidence prediction - the text contains mixed or weak sentiment signals. \"\n            \n            # Identify potential sentiment indicators\n            text_lower = text.lower()\n            \n            positive_words = ['great', 'excellent', 'amazing', 'love', 'good', 'best', 'outstanding', 'fantastic', 'wonderful']\n            negative_words = ['terrible', 'awful', 'hate', 'worst', 'bad', 'horrible', 'disappointing', 'poor']\n            \n            found_positive = [word for word in positive_words if word in text_lower]\n            found_negative = [word for word in negative_words if word in text_lower]\n            \n            if found_positive and sentiment == 'POSITIVE':\n                explanation += f\"Positive indicators detected: {', '.join(found_positive[:3])}. \"\n            elif found_negative and sentiment == 'NEGATIVE':\n                explanation += f\"Negative indicators detected: {', '.join(found_negative[:3])}. \"\n            \n            # Model information\n            explanation += f\"Analysis performed using {self.model_name} model.\"\n            \n            return explanation\n            \n        except Exception as e:\n            return f\"Error generating explanation: {str(e)}\"\n    \n    def get_sentiment_trends(self, texts_with_dates):\n        \"\"\"\n        Analyze sentiment trends over time.\n        \n        Args:\n            texts_with_dates (list): List of tuples (date, text)\n        \n        Returns:\n            pd.DataFrame: Sentiment trends over time\n        \"\"\"\n        try:\n            # Analyze all texts\n            texts = [text for date, text in texts_with_dates]\n            dates = [date for date, text in texts_with_dates]\n            \n            results = self.analyze_batch(texts)\n            \n            # Create DataFrame\n            df = pd.DataFrame({\n                'date': pd.to_datetime(dates),\n                'text': texts,\n                'sentiment': [r['sentiment'] for r in results],\n                'confidence': [r['confidence'] for r in results]\n            })\n            \n            # Aggregate by date\n            daily_sentiment = df.groupby('date').agg({\n                'sentiment': lambda x: x.mode().iloc[0] if not x.mode().empty else 'NEUTRAL',\n                'confidence': 'mean'\n            }).reset_index()\n            \n            # Calculate sentiment scores\n            sentiment_mapping = {'POSITIVE': 1, 'NEUTRAL': 0, 'NEGATIVE': -1}\n            daily_sentiment['sentiment_score'] = daily_sentiment['sentiment'].apply(lambda x: sentiment_mapping.get(x, 0))\n            \n            return daily_sentiment\n            \n        except Exception as e:\n            print(f\"Error analyzing sentiment trends: {e}\")\n            return pd.DataFrame()\n    \n    def benchmark_model_performance(self, test_texts_with_labels):\n        \"\"\"\n        Benchmark the sentiment model performance against labeled data.\n        \n        Args:\n            test_texts_with_labels (list): List of tuples (text, true_label)\n        \n        Returns:\n            dict: Performance metrics\n        \"\"\"\n        try:\n            texts = [text for text, label in test_texts_with_labels]\n            true_labels = [label for text, label in test_texts_with_labels]\n            \n            # Analyze all texts\n            results = self.analyze_batch(texts)\n            predicted_labels = [r['sentiment'] for r in results]\n            confidences = [r['confidence'] for r in results]\n            \n            # Calculate accuracy\n            correct_predictions = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n            accuracy = correct_predictions / len(true_labels)\n            \n            # Calculate per-class metrics\n            unique_labels = list(set(true_labels))\n            class_metrics = {}\n            \n            for label in unique_labels:\n                true_positives = sum(1 for true, pred in zip(true_labels, predicted_labels) \n                                   if true == label and pred == label)\n                false_positives = sum(1 for true, pred in zip(true_labels, predicted_labels) \n                                    if true != label and pred == label)\n                false_negatives = sum(1 for true, pred in zip(true_labels, predicted_labels) \n                                    if true == label and pred != label)\n                \n                precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n                recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n                \n                class_metrics[label] = {\n                    'precision': precision,\n                    'recall': recall,\n                    'f1_score': f1_score\n                }\n            \n            return {\n                'overall_accuracy': accuracy,\n                'average_confidence': np.mean(confidences),\n                'class_metrics': class_metrics,\n                'total_samples': len(test_texts_with_labels),\n                'model_name': self.model_name\n            }\n            \n        except Exception as e:\n            print(f\"Error benchmarking model: {e}\")\n            return {'error': str(e)}\n","size_bytes":19302},"attached_assets/main_1756659691747.py":{"content":"import streamlit as st\r\nfrom transformers import pipeline\r\nimport pandas as pd\r\nimport time\r\n\r\n# Configure Streamlit page\r\nst.set_page_config(\r\n    page_title=\"ðŸš€ AI Sentiment Analysis\",\r\n    page_icon=\"ðŸŽ¯\",\r\n    layout=\"wide\"\r\n)\r\n\r\n# Main title\r\nst.title(\"ðŸš€ Live AI Sentiment Analysis\")\r\nst.markdown(\"**âœ¨ Running live on Replit - Day 1 Success!**\")\r\nst.markdown(\"---\")\r\n\r\n# Load AI model with caching\r\n@st.cache_resource\r\ndef load_sentiment_model():\r\n    \"\"\"Load the sentiment analysis model (cached for performance)\"\"\"\r\n    print(\"ðŸ¤– Loading AI model...\")\r\n    return pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\r\n\r\n# Initialize the AI model\r\ntry:\r\n    with st.spinner(\"ðŸ¤– Loading AI engine... (First time takes ~30 seconds)\"):\r\n        analyzer = load_sentiment_model()\r\n    st.success(\"âœ… AI Engine Ready! Start analyzing sentiment below! ðŸŽ¯\")\r\n    \r\nexcept Exception as e:\r\n    st.error(f\"âŒ Error loading AI model: {e}\")\r\n    st.info(\"ðŸ’¡ Try refreshing the page if this persists.\")\r\n    st.stop()\r\n\r\n# Main interface\r\ncol1, col2 = st.columns([2, 1])\r\n\r\nwith col1:\r\n    st.subheader(\"ðŸ” Analyze Text Sentiment\")\r\n    \r\n    # Text input\r\n    user_text = st.text_area(\r\n        \"Enter your text to analyze:\",\r\n        height=120,\r\n        placeholder=\"Example: I absolutely love this product! The quality is amazing and delivery was super fast.\",\r\n        help=\"Enter any text - product review, social media post, comment, feedback, etc.\"\r\n    )\r\n    \r\n    # Analyze button\r\n    if st.button(\"ðŸš€ Analyze Sentiment\", type=\"primary\", use_container_width=True):\r\n        if user_text.strip():\r\n            # Show analysis in progress\r\n            with st.spinner(\"ðŸ” Analyzing sentiment...\"):\r\n                start_time = time.time()\r\n                \r\n                # Get AI prediction\r\n                result = analyzer(user_text)[0]\r\n                \r\n                # Calculate processing time\r\n                processing_time = time.time() - start_time\r\n            \r\n            # Display results\r\n            st.markdown(\"### ðŸ“Š Analysis Results\")\r\n            \r\n            # Main metrics\r\n            col_a, col_b, col_c = st.columns(3)\r\n            \r\n            # Determine emoji and color\r\n            emoji = \"ðŸ˜Š\" if result[\"label\"] == \"POSITIVE\" else \"ðŸ˜ž\"\r\n            confidence_pct = result[\"score\"] * 100\r\n            \r\n            with col_a:\r\n                if result[\"label\"] == \"POSITIVE\":\r\n                    st.success(f\"ðŸ˜Š **POSITIVE**\")\r\n                else:\r\n                    st.error(f\"ðŸ˜ž **NEGATIVE**\")\r\n            \r\n            with col_b:\r\n                st.metric(\"Confidence Score\", f\"{confidence_pct:.1f}%\")\r\n            \r\n            with col_c:\r\n                st.metric(\"Processing Time\", f\"{processing_time:.2f}s\")\r\n            \r\n            # Confidence visualization\r\n            st.markdown(\"**Confidence Level:**\")\r\n            st.progress(result[\"score\"])\r\n            \r\n            # Text stats\r\n            col_x, col_y, col_z = st.columns(3)\r\n            \r\n            with col_x:\r\n                st.metric(\"Text Length\", f\"{len(user_text)} characters\")\r\n            \r\n            with col_y:\r\n                word_count = len(user_text.split())\r\n                st.metric(\"Word Count\", f\"{word_count} words\")\r\n            \r\n            with col_z:\r\n                confidence_level = \"High\" if result[\"score\"] >= 0.8 else \"Medium\" if result[\"score\"] >= 0.6 else \"Low\"\r\n                st.metric(\"Confidence Level\", confidence_level)\r\n            \r\n            # Detailed explanation\r\n            if result[\"label\"] == \"POSITIVE\":\r\n                st.info(f\"ðŸŽ‰ **Great news!** This text expresses positive sentiment with **{confidence_pct:.1f}%** confidence. The AI detected positive language patterns and emotional indicators.\")\r\n            else:\r\n                st.warning(f\"âš ï¸ **Heads up!** This text expresses negative sentiment with **{confidence_pct:.1f}%** confidence. The AI detected negative language patterns and emotional indicators.\")\r\n                \r\n        else:\r\n            st.warning(\"âš ï¸ Please enter some text to analyze!\")\r\n\r\nwith col2:\r\n    st.subheader(\"ðŸ’¡ Quick Examples\")\r\n    st.markdown(\"*Click to try these examples:*\")\r\n    \r\n    examples = [\r\n        (\"ðŸ˜Š Positive\", \"This product is absolutely incredible! Best purchase I've ever made. Amazing quality and super fast delivery!\"),\r\n        (\"ðŸ˜ž Negative\", \"Terrible experience! The product broke after one day and customer service was completely unhelpful. Total waste of money.\"),\r\n        (\"ðŸ˜ Mixed\", \"The product works fine and delivery was on time, but nothing extraordinary. Just average quality for the price.\"),\r\n        (\"ðŸ¤ Service\", \"Outstanding customer support! They responded within minutes and solved my problem perfectly. Highly recommend!\")\r\n    ]\r\n    \r\n    for label, example in examples:\r\n        if st.button(label, use_container_width=True):\r\n            st.session_state.selected_example = example\r\n\r\n# Handle example selection\r\nif hasattr(st.session_state, 'selected_example'):\r\n    st.rerun()\r\n\r\n# Batch Analysis Section\r\nst.markdown(\"---\")\r\nst.subheader(\"ðŸ“Š Batch Analysis - Multiple Texts\")\r\n\r\nbatch_input = st.text_area(\r\n    \"Enter multiple texts (one per line):\",\r\n    height=120,\r\n    placeholder=\"\"\"I love this amazing product!\r\nTerrible quality, very disappointed.\r\nThe service was decent, nothing special.\r\nOutstanding experience, highly recommend!\r\nPoor value for money, not worth it.\"\"\"\r\n)\r\n\r\ncol_batch1, col_batch2 = st.columns(2)\r\n\r\nwith col_batch1:\r\n    if st.button(\"ðŸ“Š Analyze All Texts\", type=\"secondary\"):\r\n        if batch_input.strip():\r\n            # Parse texts\r\n            texts = [line.strip() for line in batch_input.split('\\n') if line.strip()]\r\n            \r\n            if texts:\r\n                # Show progress\r\n                progress_bar = st.progress(0)\r\n                status_text = st.empty()\r\n                \r\n                results = []\r\n                \r\n                # Analyze each text\r\n                for i, text in enumerate(texts):\r\n                    status_text.text(f\"Analyzing text {i+1} of {len(texts)}...\")\r\n                    \r\n                    # Get prediction\r\n                    result = analyzer(text)[0]\r\n                    \r\n                    # Store result\r\n                    emoji = \"ðŸ˜Š\" if result[\"label\"] == \"POSITIVE\" else \"ðŸ˜ž\"\r\n                    results.append({\r\n                        \"Text\": text[:60] + \"...\" if len(text) > 60 else text,\r\n                        \"Sentiment\": f\"{emoji} {result['label']}\",\r\n                        \"Confidence\": f\"{result['score']:.1%}\",\r\n                        \"Full_Text\": text,\r\n                        \"Raw_Score\": result['score']\r\n                    })\r\n                    \r\n                    # Update progress\r\n                    progress_bar.progress((i + 1) / len(texts))\r\n                \r\n                # Clear progress indicators\r\n                progress_bar.empty()\r\n                status_text.empty()\r\n                \r\n                # Show summary statistics\r\n                st.markdown(\"### ðŸ“ˆ Summary Statistics\")\r\n                \r\n                positive_count = sum(1 for r in results if \"POSITIVE\" in r[\"Sentiment\"])\r\n                negative_count = len(results) - positive_count\r\n                avg_confidence = sum(r[\"Raw_Score\"] for r in results) / len(results)\r\n                \r\n                col_s1, col_s2, col_s3, col_s4 = st.columns(4)\r\n                \r\n                with col_s1:\r\n                    st.metric(\"Total Texts\", len(results))\r\n                \r\n                with col_s2:\r\n                    st.metric(\"ðŸ˜Š Positive\", f\"{positive_count} ({positive_count/len(results)*100:.1f}%)\")\r\n                \r\n                with col_s3:\r\n                    st.metric(\"ðŸ˜ž Negative\", f\"{negative_count} ({negative_count/len(results)*100:.1f}%)\")\r\n                \r\n                with col_s4:\r\n                    st.metric(\"Avg Confidence\", f\"{avg_confidence:.1%}\")\r\n                \r\n                # Results table\r\n                st.markdown(\"### ðŸ“‹ Detailed Results\")\r\n                \r\n                display_df = pd.DataFrame([{\r\n                    'Text Preview': r['Text'],\r\n                    'Sentiment': r['Sentiment'],\r\n                    'Confidence': r['Confidence']\r\n                } for r in results])\r\n                \r\n                st.dataframe(display_df, use_container_width=True, hide_index=True)\r\n                \r\n                # Download option\r\n                full_df = pd.DataFrame([{\r\n                    'Text': r['Full_Text'],\r\n                    'Sentiment': r['Sentiment'].split()[-1],  # Remove emoji\r\n                    'Confidence_Score': r['Raw_Score'],\r\n                    'Confidence_Percentage': r['Confidence']\r\n                } for r in results])\r\n                \r\n                csv = full_df.to_csv(index=False)\r\n                st.download_button(\r\n                    label=\"ðŸ“¥ Download Results as CSV\",\r\n                    data=csv,\r\n                    file_name=\"sentiment_analysis_results.csv\",\r\n                    mime=\"text/csv\"\r\n                )\r\n                \r\n            else:\r\n                st.warning(\"âš ï¸ No valid texts found. Please check your input.\")\r\n        else:\r\n            st.warning(\"âš ï¸ Please enter some texts to analyze.\")\r\n\r\nwith col_batch2:\r\n    st.info(\"ðŸ’¡ **Batch Analysis Tips:**\\n\\nâ€¢ Enter one text per line\\nâ€¢ Mix positive and negative examples\\nâ€¢ Check the summary statistics\\nâ€¢ Download results as CSV\")\r\n\r\n# Footer and sharing info\r\nst.markdown(\"---\")\r\nst.markdown(\"\"\"\r\n### ðŸŽ‰ Congratulations! Your AI App is LIVE!\r\n\r\n**ðŸŒ Share this app:** Your Replit automatically created a public URL - click the share button in Replit to get the link!\r\n\r\n**ðŸ“± Mobile friendly:** This app works perfectly on phones and tablets too!\r\n\r\n**ðŸ”„ Always online:** Your app is hosted in the cloud and accessible 24/7!\r\n\"\"\")\r\n\r\n# Sidebar information\r\nst.sidebar.title(\"ðŸ“Š App Information\")\r\n\r\nst.sidebar.markdown(\"\"\"\r\n### ðŸš€ Live on Replit!\r\n\r\n**âœ¨ Features:**\r\n- Real-time AI sentiment analysis\r\n- Single text processing\r\n- Batch analysis capabilities  \r\n- CSV data export\r\n- Mobile-responsive design\r\n\r\n**ðŸ¤– AI Model:**\r\n- **Engine:** HuggingFace Transformers\r\n- **Model:** DistilBERT\r\n- **Accuracy:** ~91% on standard datasets\r\n- **Speed:** <100ms per analysis\r\n\r\n**ðŸ“Š Project Status:**\r\n- âœ… Day 1: Complete!\r\n- ðŸ”„ Day 2: Real-time streaming\r\n- ðŸ“ˆ Day 3: Advanced charts\r\n- ðŸŽ¨ Day 4: Enhanced UI\r\n- ðŸŒ Day 5: Full deployment\r\n\"\"\")\r\n\r\nst.sidebar.markdown(\"\"\"\r\n---\r\n**ðŸŽ¯ Built in:** Python + Streamlit  \r\n**ðŸ”¥ Hosted on:** Replit  \r\n**â° Started:** Aug 31, 2025, 10:29 PM IST  \r\n**ðŸ† Status:** LIVE and WORKING! ðŸš€\r\n\"\"\")\r\n","size_bytes":10923},"utils/data_preprocessor.py":{"content":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass DataPreprocessor:\n    \"\"\"\n    Comprehensive data preprocessing module for market trend analysis.\n    Handles data cleaning, feature engineering, and preparation for various AI models.\n    \"\"\"\n    \n    def __init__(self):\n        self.scalers = {}\n        self.encoders = {}\n        self.imputers = {}\n        self.feature_stats = {}\n        \n    def prepare_time_series_data(self, df, date_column=None, target_column=None, freq='D'):\n        \"\"\"\n        Prepare data for time series analysis and forecasting.\n        \n        Args:\n            df (pd.DataFrame): Input dataframe\n            date_column (str): Name of date column, if None tries to auto-detect\n            target_column (str): Target variable for forecasting\n            freq (str): Frequency for time series resampling\n        \n        Returns:\n            pd.DataFrame: Preprocessed time series data\n        \"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Auto-detect date column if not specified\n            if date_column is None:\n                date_column = self._detect_date_column(df_processed)\n            \n            if date_column is None:\n                # Create synthetic date index\n                df_processed.index = pd.date_range(\n                    start='2022-01-01', \n                    periods=len(df_processed), \n                    freq=freq\n                )\n            else:\n                # Convert date column to datetime and set as index\n                df_processed[date_column] = pd.to_datetime(df_processed[date_column])\n                df_processed.set_index(date_column, inplace=True)\n                df_processed.sort_index(inplace=True)\n            \n            # Handle duplicate timestamps\n            df_processed = df_processed[~df_processed.index.duplicated(keep='first')]\n            \n            # Resample to ensure consistent frequency\n            if freq != 'infer':\n                numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n                if len(numeric_columns) > 0:\n                    df_processed = df_processed.resample(freq).agg({\n                        **{col: 'mean' for col in numeric_columns},\n                        **{col: 'first' for col in df_processed.columns if col not in numeric_columns}\n                    })\n            \n            # Fill missing values in time series\n            df_processed = self._handle_time_series_missing_values(df_processed)\n            \n            # Remove extreme outliers\n            df_processed = self._handle_time_series_outliers(df_processed)\n            \n            # Create time-based features\n            df_processed = self._create_time_features(df_processed)\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error preparing time series data: {e}\")\n            return df\n    \n    def prepare_clustering_data(self, df, categorical_encoding='label', scaling_method='standard'):\n        \"\"\"\n        Prepare data for clustering analysis.\n        \n        Args:\n            df (pd.DataFrame): Input dataframe\n            categorical_encoding (str): Method for encoding categorical variables ('label', 'onehot')\n            scaling_method (str): Scaling method ('standard', 'minmax', 'none')\n        \n        Returns:\n            pd.DataFrame: Preprocessed data ready for clustering\n        \"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Handle missing values\n            df_processed = self._handle_missing_values(df_processed, strategy='median')\n            \n            # Encode categorical variables\n            df_processed = self._encode_categorical_variables(df_processed, method=categorical_encoding)\n            \n            # Scale numerical features\n            if scaling_method != 'none':\n                df_processed = self._scale_features(df_processed, method=scaling_method)\n            \n            # Remove highly correlated features\n            df_processed = self._remove_high_correlation_features(df_processed, threshold=0.95)\n            \n            # Remove constant features\n            df_processed = self._remove_constant_features(df_processed)\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error preparing clustering data: {e}\")\n            return df\n    \n    def prepare_sentiment_data(self, texts, max_length=512):\n        \"\"\"\n        Prepare text data for sentiment analysis.\n        \n        Args:\n            texts (list or pd.Series): Text data\n            max_length (int): Maximum text length\n        \n        Returns:\n            list: Preprocessed text data\n        \"\"\"\n        try:\n            if isinstance(texts, pd.Series):\n                texts = texts.tolist()\n            \n            processed_texts = []\n            \n            for text in texts:\n                # Convert to string\n                text = str(text)\n                \n                # Basic text cleaning\n                cleaned_text = self._clean_text(text)\n                \n                # Truncate if too long\n                if len(cleaned_text) > max_length:\n                    cleaned_text = cleaned_text[:max_length]\n                \n                processed_texts.append(cleaned_text)\n            \n            return processed_texts\n            \n        except Exception as e:\n            print(f\"Error preparing sentiment data: {e}\")\n            return texts\n    \n    def create_features_for_forecasting(self, df, target_column, lag_features=True, rolling_features=True, \n                                      seasonal_features=True, lag_periods=[1, 7, 30], \n                                      rolling_windows=[7, 14, 30]):\n        \"\"\"\n        Create features for machine learning-based forecasting.\n        \n        Args:\n            df (pd.DataFrame): Time series dataframe\n            target_column (str): Target variable column name\n            lag_features (bool): Create lag features\n            rolling_features (bool): Create rolling statistics features\n            seasonal_features (bool): Create seasonal features\n            lag_periods (list): List of lag periods to create\n            rolling_windows (list): List of rolling window sizes\n        \n        Returns:\n            pd.DataFrame: Feature-engineered dataframe\n        \"\"\"\n        try:\n            df_features = df.copy()\n            \n            if target_column not in df_features.columns:\n                raise ValueError(f\"Target column '{target_column}' not found in dataframe\")\n            \n            # Create lag features\n            if lag_features:\n                for lag in lag_periods:\n                    df_features[f'{target_column}_lag_{lag}'] = df_features[target_column].shift(lag)\n            \n            # Create rolling statistics features\n            if rolling_features:\n                for window in rolling_windows:\n                    df_features[f'{target_column}_rolling_mean_{window}'] = (\n                        df_features[target_column].rolling(window=window).mean()\n                    )\n                    df_features[f'{target_column}_rolling_std_{window}'] = (\n                        df_features[target_column].rolling(window=window).std()\n                    )\n                    df_features[f'{target_column}_rolling_min_{window}'] = (\n                        df_features[target_column].rolling(window=window).min()\n                    )\n                    df_features[f'{target_column}_rolling_max_{window}'] = (\n                        df_features[target_column].rolling(window=window).max()\n                    )\n            \n            # Create seasonal features\n            if seasonal_features:\n                df_features = self._create_seasonal_features(df_features, target_column)\n            \n            # Create difference features\n            df_features[f'{target_column}_diff_1'] = df_features[target_column].diff(1)\n            df_features[f'{target_column}_diff_7'] = df_features[target_column].diff(7)\n            \n            # Create percentage change features\n            df_features[f'{target_column}_pct_change_1'] = df_features[target_column].pct_change(1)\n            df_features[f'{target_column}_pct_change_7'] = df_features[target_column].pct_change(7)\n            \n            # Drop rows with NaN values created by feature engineering\n            df_features.dropna(inplace=True)\n            \n            return df_features\n            \n        except Exception as e:\n            print(f\"Error creating forecasting features: {e}\")\n            return df\n    \n    def _detect_date_column(self, df):\n        \"\"\"Auto-detect date column in dataframe\"\"\"\n        try:\n            # Check for common date column names\n            date_keywords = ['date', 'time', 'timestamp', 'day', 'created', 'updated']\n            \n            for col in df.columns:\n                if any(keyword in col.lower() for keyword in date_keywords):\n                    try:\n                        pd.to_datetime(df[col].head())\n                        return col\n                    except:\n                        continue\n            \n            # Check for datetime types\n            datetime_cols = df.select_dtypes(include=['datetime64']).columns\n            if len(datetime_cols) > 0:\n                return datetime_cols[0]\n            \n            # Try to convert object columns to datetime\n            for col in df.select_dtypes(include=['object']).columns:\n                try:\n                    pd.to_datetime(df[col].head())\n                    return col\n                except:\n                    continue\n            \n            return None\n            \n        except Exception as e:\n            print(f\"Error detecting date column: {e}\")\n            return None\n    \n    def _handle_missing_values(self, df, strategy='median'):\n        \"\"\"Handle missing values in dataframe\"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Separate numeric and categorical columns\n            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n            categorical_cols = df_processed.select_dtypes(include=['object']).columns\n            \n            # Handle numeric missing values\n            if len(numeric_cols) > 0:\n                if strategy == 'knn':\n                    imputer = KNNImputer(n_neighbors=5)\n                else:\n                    imputer = SimpleImputer(strategy=strategy)\n                \n                df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])\n                self.imputers['numeric'] = imputer\n            \n            # Handle categorical missing values\n            if len(categorical_cols) > 0:\n                imputer = SimpleImputer(strategy='most_frequent')\n                df_processed[categorical_cols] = imputer.fit_transform(df_processed[categorical_cols])\n                self.imputers['categorical'] = imputer\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error handling missing values: {e}\")\n            return df\n    \n    def _handle_time_series_missing_values(self, df):\n        \"\"\"Handle missing values specifically for time series data\"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Forward fill first, then backward fill\n            df_processed = df_processed.fillna(method='ffill').fillna(method='bfill')\n            \n            # For remaining NaN values, use interpolation\n            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n            df_processed[numeric_cols] = df_processed[numeric_cols].interpolate(method='linear')\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error handling time series missing values: {e}\")\n            return df\n    \n    def _handle_time_series_outliers(self, df, method='iqr', factor=1.5):\n        \"\"\"Handle outliers in time series data\"\"\"\n        try:\n            df_processed = df.copy()\n            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n            \n            for col in numeric_cols:\n                if method == 'iqr':\n                    Q1 = df_processed[col].quantile(0.25)\n                    Q3 = df_processed[col].quantile(0.75)\n                    IQR = Q3 - Q1\n                    lower_bound = Q1 - factor * IQR\n                    upper_bound = Q3 + factor * IQR\n                    \n                    # Cap outliers instead of removing them\n                    df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)\n                \n                elif method == 'zscore':\n                    z_scores = np.abs((df_processed[col] - df_processed[col].mean()) / df_processed[col].std())\n                    df_processed = df_processed[z_scores < 3]\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error handling outliers: {e}\")\n            return df\n    \n    def _create_time_features(self, df):\n        \"\"\"Create time-based features from datetime index\"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Ensure index is datetime\n            if not isinstance(df_processed.index, pd.DatetimeIndex):\n                return df_processed\n            \n            # Create time features\n            dt_index = pd.to_datetime(df_processed.index)\n            df_processed['year'] = dt_index.year\n            df_processed['month'] = dt_index.month\n            df_processed['day'] = dt_index.day\n            df_processed['dayofweek'] = dt_index.dayofweek\n            df_processed['dayofyear'] = dt_index.dayofyear\n            df_processed['week'] = dt_index.isocalendar().week\n            df_processed['quarter'] = dt_index.quarter\n            df_processed['is_weekend'] = (dt_index.dayofweek >= 5).astype(int)\n            df_processed['is_month_start'] = dt_index.is_month_start.astype(int)\n            df_processed['is_month_end'] = dt_index.is_month_end.astype(int)\n            df_processed['is_quarter_start'] = dt_index.is_quarter_start.astype(int)\n            df_processed['is_quarter_end'] = dt_index.is_quarter_end.astype(int)\n            \n            # Create cyclical features\n            df_processed['month_sin'] = np.sin(2 * np.pi * df_processed['month'] / 12)\n            df_processed['month_cos'] = np.cos(2 * np.pi * df_processed['month'] / 12)\n            df_processed['dayofweek_sin'] = np.sin(2 * np.pi * df_processed['dayofweek'] / 7)\n            df_processed['dayofweek_cos'] = np.cos(2 * np.pi * df_processed['dayofweek'] / 7)\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error creating time features: {e}\")\n            return df\n    \n    def _encode_categorical_variables(self, df, method='label'):\n        \"\"\"Encode categorical variables\"\"\"\n        try:\n            df_processed = df.copy()\n            categorical_cols = df_processed.select_dtypes(include=['object']).columns\n            \n            for col in categorical_cols:\n                if method == 'label':\n                    encoder = LabelEncoder()\n                    df_processed[col] = encoder.fit_transform(df_processed[col].astype(str))\n                    self.encoders[col] = encoder\n                \n                elif method == 'onehot':\n                    # Only use one-hot encoding for columns with reasonable number of categories\n                    if df_processed[col].nunique() <= 10:\n                        encoded_cols = pd.get_dummies(df_processed[col], prefix=col)\n                        df_processed = pd.concat([df_processed.drop(col, axis=1), encoded_cols], axis=1)\n                    else:\n                        # Fall back to label encoding for high cardinality\n                        encoder = LabelEncoder()\n                        df_processed[col] = encoder.fit_transform(df_processed[col].astype(str))\n                        self.encoders[col] = encoder\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error encoding categorical variables: {e}\")\n            return df\n    \n    def _scale_features(self, df, method='standard'):\n        \"\"\"Scale numerical features\"\"\"\n        try:\n            df_processed = df.copy()\n            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n            \n            if len(numeric_cols) == 0:\n                return df_processed\n            \n            if method == 'standard':\n                scaler = StandardScaler()\n            elif method == 'minmax':\n                scaler = MinMaxScaler()\n            else:\n                return df_processed\n            \n            df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])\n            self.scalers['features'] = scaler\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error scaling features: {e}\")\n            return df\n    \n    def _remove_high_correlation_features(self, df, threshold=0.95):\n        \"\"\"Remove highly correlated features\"\"\"\n        try:\n            df_processed = df.copy()\n            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n            \n            if len(numeric_cols) < 2:\n                return df_processed\n            \n            # Calculate correlation matrix\n            corr_matrix = df_processed[numeric_cols].corr().abs()\n            \n            # Find highly correlated pairs\n            upper_triangle = corr_matrix.where(\n                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n            )\n            \n            # Find features to drop\n            to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n            \n            # Drop highly correlated features\n            df_processed = df_processed.drop(columns=to_drop)\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error removing correlated features: {e}\")\n            return df\n    \n    def _remove_constant_features(self, df):\n        \"\"\"Remove features with constant values\"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Find constant features\n            constant_features = [col for col in df_processed.columns if df_processed[col].nunique() <= 1]\n            \n            # Drop constant features\n            df_processed = df_processed.drop(columns=constant_features)\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error removing constant features: {e}\")\n            return df\n    \n    def _clean_text(self, text):\n        \"\"\"Clean text for sentiment analysis\"\"\"\n        import re\n        \n        try:\n            # Convert to string\n            text = str(text)\n            \n            # Remove URLs\n            text = re.sub(r'http\\S+|www.\\S+', '', text)\n            \n            # Remove special characters but keep basic punctuation\n            text = re.sub(r'[^\\w\\s.,!?;:-]', '', text)\n            \n            # Remove extra whitespace\n            text = re.sub(r'\\s+', ' ', text)\n            \n            # Strip leading/trailing whitespace\n            text = text.strip()\n            \n            return text\n            \n        except Exception as e:\n            print(f\"Error cleaning text: {e}\")\n            return str(text)\n    \n    def _create_seasonal_features(self, df, target_column):\n        \"\"\"Create seasonal decomposition features\"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Create seasonal indicators\n            df_processed['is_holiday_season'] = ((df_processed.index.month == 12) | \n                                               (df_processed.index.month == 1)).astype(int)\n            df_processed['is_summer'] = ((df_processed.index.month >= 6) & \n                                       (df_processed.index.month <= 8)).astype(int)\n            df_processed['is_winter'] = ((df_processed.index.month >= 12) | \n                                       (df_processed.index.month <= 2)).astype(int)\n            \n            # Create seasonal moving averages\n            df_processed[f'{target_column}_seasonal_avg'] = (\n                df_processed.groupby(df_processed.index.month)[target_column].transform('mean')\n            )\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error creating seasonal features: {e}\")\n            return df\n    \n    def get_preprocessing_summary(self):\n        \"\"\"Get summary of preprocessing operations performed\"\"\"\n        summary = {\n            'scalers_used': list(self.scalers.keys()),\n            'encoders_used': list(self.encoders.keys()),\n            'imputers_used': list(self.imputers.keys()),\n            'feature_stats': self.feature_stats\n        }\n        return summary\n    \n    def transform_new_data(self, df, data_type='time_series'):\n        \"\"\"Transform new data using fitted preprocessors\"\"\"\n        try:\n            df_processed = df.copy()\n            \n            # Apply saved transformations based on data type\n            if data_type == 'time_series':\n                df_processed = self.prepare_time_series_data(df_processed)\n            elif data_type == 'clustering':\n                df_processed = self.prepare_clustering_data(df_processed)\n            \n            return df_processed\n            \n        except Exception as e:\n            print(f\"Error transforming new data: {e}\")\n            return df\n","size_bytes":22063},"utils/visualization.py":{"content":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef create_trend_chart(historical_data, forecast_data, title=\"Market Trend Analysis\", confidence_intervals=None):\n    \"\"\"\n    Create an interactive trend chart showing historical data and forecasts.\n    \n    Args:\n        historical_data (pd.Series): Historical time series data\n        forecast_data (pd.Series): Forecast time series data\n        title (str): Chart title\n        confidence_intervals (dict): Optional confidence intervals with 'lower' and 'upper' keys\n    \n    Returns:\n        plotly.graph_objects.Figure: Interactive trend chart\n    \"\"\"\n    try:\n        fig = go.Figure()\n        \n        # Add historical data\n        if historical_data is not None and not historical_data.empty:\n            fig.add_trace(go.Scatter(\n                x=historical_data.index,\n                y=historical_data.values,\n                mode='lines',\n                name='Historical Data',\n                line=dict(color='#1f77b4', width=2),\n                hovertemplate='<b>Date:</b> %{x}<br><b>Value:</b> %{y:.2f}<extra></extra>'\n            ))\n        \n        # Add forecast data\n        if forecast_data is not None and not forecast_data.empty:\n            fig.add_trace(go.Scatter(\n                x=forecast_data.index,\n                y=forecast_data.values,\n                mode='lines',\n                name='Forecast',\n                line=dict(color='#ff7f0e', width=2, dash='dash'),\n                hovertemplate='<b>Date:</b> %{x}<br><b>Forecast:</b> %{y:.2f}<extra></extra>'\n            ))\n        \n        # Add confidence intervals if provided\n        if confidence_intervals and 'lower' in confidence_intervals and 'upper' in confidence_intervals:\n            # Upper bound\n            fig.add_trace(go.Scatter(\n                x=confidence_intervals['upper'].index,\n                y=confidence_intervals['upper'].values,\n                mode='lines',\n                name='Upper Confidence',\n                line=dict(color='rgba(255, 127, 14, 0.3)', width=0),\n                showlegend=False,\n                hoverinfo='skip'\n            ))\n            \n            # Lower bound with fill\n            fig.add_trace(go.Scatter(\n                x=confidence_intervals['lower'].index,\n                y=confidence_intervals['lower'].values,\n                mode='lines',\n                name='Confidence Interval',\n                line=dict(color='rgba(255, 127, 14, 0.3)', width=0),\n                fill='tonexty',\n                fillcolor='rgba(255, 127, 14, 0.2)',\n                hovertemplate='<b>Date:</b> %{x}<br><b>Lower:</b> %{y:.2f}<extra></extra>'\n            ))\n        \n        # Update layout\n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 20}\n            },\n            xaxis_title=\"Date\",\n            yaxis_title=\"Value\",\n            hovermode='x unified',\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.02,\n                xanchor=\"right\",\n                x=1\n            ),\n            template='plotly_white',\n            height=500\n        )\n        \n        # Add annotations for forecast start\n        if historical_data is not None and forecast_data is not None and not historical_data.empty and not forecast_data.empty:\n            forecast_start = forecast_data.index[0]\n            fig.add_vline(\n                x=forecast_start,\n                line_dash=\"dot\",\n                line_color=\"gray\",\n                annotation_text=\"Forecast Start\",\n                annotation_position=\"top\"\n            )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating trend chart: {e}\")\n        # Return empty figure as fallback\n        return go.Figure().add_annotation(\n            text=f\"Error creating chart: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_cluster_visualization(data, labels, feature_names=None, title=\"Customer Segmentation\"):\n    \"\"\"\n    Create a scatter plot visualization for clustering results.\n    \n    Args:\n        data (pd.DataFrame): Feature data used for clustering\n        labels (pd.Series): Cluster labels\n        feature_names (list): Names of features to plot (uses first 2 if not specified)\n        title (str): Chart title\n    \n    Returns:\n        plotly.graph_objects.Figure: Interactive cluster visualization\n    \"\"\"\n    try:\n        # Ensure we have at least 2 features\n        if data.shape[1] < 2:\n            # Add a synthetic second dimension\n            data = data.copy()\n            data['synthetic_feature'] = np.random.normal(0, 1, len(data))\n        \n        # Select features to plot\n        if feature_names is None or len(feature_names) < 2:\n            feature_names = data.columns[:2].tolist()\n        \n        x_feature = feature_names[0]\n        y_feature = feature_names[1]\n        \n        # Create DataFrame for plotting\n        plot_data = pd.DataFrame({\n            'x': data[x_feature],\n            'y': data[y_feature],\n            'cluster': labels.astype(str)\n        })\n        \n        # Create scatter plot\n        fig = px.scatter(\n            plot_data,\n            x='x',\n            y='y',\n            color='cluster',\n            title=title,\n            labels={'x': x_feature, 'y': y_feature, 'cluster': 'Cluster'},\n            hover_data={'cluster': True}\n        )\n        \n        # Update traces for better styling\n        fig.update_traces(\n            marker=dict(size=8, opacity=0.7),\n            hovertemplate=f'<b>{x_feature}:</b> %{{x:.2f}}<br><b>{y_feature}:</b> %{{y:.2f}}<br><b>Cluster:</b> %{{customdata[0]}}<extra></extra>'\n        )\n        \n        # Update layout\n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 18}\n            },\n            template='plotly_white',\n            height=500,\n            legend=dict(\n                title=\"Cluster\",\n                orientation=\"v\",\n                yanchor=\"top\",\n                y=1,\n                xanchor=\"left\",\n                x=1.02\n            )\n        )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating cluster visualization: {e}\")\n        # Return empty figure as fallback\n        return go.Figure().add_annotation(\n            text=f\"Error creating cluster visualization: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_sentiment_distribution_chart(sentiment_data, title=\"Sentiment Distribution\"):\n    \"\"\"\n    Create a pie chart showing sentiment distribution.\n    \n    Args:\n        sentiment_data (pd.Series or dict): Sentiment counts or distribution\n        title (str): Chart title\n    \n    Returns:\n        plotly.graph_objects.Figure: Sentiment distribution pie chart\n    \"\"\"\n    try:\n        if isinstance(sentiment_data, dict):\n            labels = list(sentiment_data.keys())\n            values = list(sentiment_data.values())\n        else:\n            sentiment_counts = sentiment_data.value_counts()\n            labels = sentiment_counts.index.tolist()\n            values = sentiment_counts.values.tolist()\n        \n        # Define colors for sentiments\n        color_map = {\n            'POSITIVE': '#2E8B57',  # Sea Green\n            'NEGATIVE': '#DC143C',  # Crimson\n            'NEUTRAL': '#FFD700',   # Gold\n            'BULLISH': '#2E8B57',   # Sea Green\n            'BEARISH': '#DC143C',   # Crimson\n            'MIXED': '#FFD700'      # Gold\n        }\n        \n        colors = [color_map.get(label, '#1f77b4') for label in labels]\n        \n        fig = go.Figure(data=[go.Pie(\n            labels=labels,\n            values=values,\n            hole=0.3,\n            marker=dict(colors=colors),\n            hovertemplate='<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent}<extra></extra>'\n        )])\n        \n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 18}\n            },\n            template='plotly_white',\n            height=400,\n            showlegend=True,\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=-0.1,\n                xanchor=\"center\",\n                x=0.5\n            )\n        )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating sentiment distribution chart: {e}\")\n        return go.Figure().add_annotation(\n            text=f\"Error creating sentiment chart: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_time_series_decomposition_chart(decomposition_data, title=\"Time Series Decomposition\"):\n    \"\"\"\n    Create a multi-panel chart showing time series decomposition components.\n    \n    Args:\n        decomposition_data (dict): Dictionary with 'trend', 'seasonal', 'residual', 'observed' components\n        title (str): Chart title\n    \n    Returns:\n        plotly.graph_objects.Figure: Time series decomposition chart\n    \"\"\"\n    try:\n        # Create subplots\n        fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residual'),\n            vertical_spacing=0.08,\n            shared_xaxes=True\n        )\n        \n        components = ['observed', 'trend', 'seasonal', 'residual']\n        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n        \n        for i, (component, color) in enumerate(zip(components, colors), 1):\n            if component in decomposition_data and decomposition_data[component] is not None:\n                data = decomposition_data[component]\n                fig.add_trace(\n                    go.Scatter(\n                        x=data.index,\n                        y=data.values,\n                        mode='lines',\n                        name=component.title(),\n                        line=dict(color=color, width=1.5),\n                        showlegend=False\n                    ),\n                    row=i, col=1\n                )\n        \n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 18}\n            },\n            height=800,\n            template='plotly_white'\n        )\n        \n        # Update x-axis for bottom subplot only\n        fig.update_xaxes(title_text=\"Date\", row=4, col=1)\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating decomposition chart: {e}\")\n        return go.Figure().add_annotation(\n            text=f\"Error creating decomposition chart: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_feature_importance_chart(importance_data, title=\"Feature Importance\"):\n    \"\"\"\n    Create a horizontal bar chart showing feature importance.\n    \n    Args:\n        importance_data (dict): Dictionary with feature names as keys and importance scores as values\n        title (str): Chart title\n    \n    Returns:\n        plotly.graph_objects.Figure: Feature importance bar chart\n    \"\"\"\n    try:\n        features = list(importance_data.keys())\n        importances = list(importance_data.values())\n        \n        # Sort by importance\n        sorted_data = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)\n        features_sorted = [x[0] for x in sorted_data]\n        importances_sorted = [x[1] for x in sorted_data]\n        \n        fig = go.Figure(data=[\n            go.Bar(\n                x=importances_sorted,\n                y=features_sorted,\n                orientation='h',\n                marker=dict(color='#1f77b4'),\n                hovertemplate='<b>Feature:</b> %{y}<br><b>Importance:</b> %{x:.4f}<extra></extra>'\n            )\n        ])\n        \n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 18}\n            },\n            xaxis_title=\"Importance Score\",\n            yaxis_title=\"Features\",\n            template='plotly_white',\n            height=max(400, len(features) * 30)\n        )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating feature importance chart: {e}\")\n        return go.Figure().add_annotation(\n            text=f\"Error creating feature importance chart: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_correlation_heatmap(data, title=\"Feature Correlation Heatmap\"):\n    \"\"\"\n    Create a correlation heatmap for numerical features.\n    \n    Args:\n        data (pd.DataFrame): Data for correlation analysis\n        title (str): Chart title\n    \n    Returns:\n        plotly.graph_objects.Figure: Correlation heatmap\n    \"\"\"\n    try:\n        # Calculate correlation matrix\n        numeric_data = data.select_dtypes(include=[np.number])\n        \n        if numeric_data.empty:\n            raise ValueError(\"No numerical data available for correlation analysis\")\n        \n        corr_matrix = numeric_data.corr()\n        \n        # Create heatmap\n        fig = go.Figure(data=go.Heatmap(\n            z=corr_matrix.values,\n            x=corr_matrix.columns,\n            y=corr_matrix.columns,\n            colorscale='RdBu',\n            zmid=0,\n            hovertemplate='<b>%{y}</b> vs <b>%{x}</b><br>Correlation: %{z:.3f}<extra></extra>'\n        ))\n        \n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 18}\n            },\n            template='plotly_white',\n            height=500,\n            width=500\n        )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating correlation heatmap: {e}\")\n        return go.Figure().add_annotation(\n            text=f\"Error creating correlation heatmap: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_model_performance_chart(metrics_data, title=\"Model Performance Comparison\"):\n    \"\"\"\n    Create a radar chart comparing model performance metrics.\n    \n    Args:\n        metrics_data (dict): Dictionary with model names as keys and metrics as values\n        title (str): Chart title\n    \n    Returns:\n        plotly.graph_objects.Figure: Model performance radar chart\n    \"\"\"\n    try:\n        fig = go.Figure()\n        \n        # Define metrics to display (normalize to 0-1 scale)\n        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n        \n        for model_name, metrics in metrics_data.items():\n            # Extract and normalize metrics\n            values = []\n            for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n                if metric in metrics:\n                    values.append(metrics[metric])\n                else:\n                    values.append(0)\n            \n            # Add the first value at the end to close the radar chart\n            values.append(values[0])\n            metric_names_closed = metric_names + [metric_names[0]]\n            \n            fig.add_trace(go.Scatterpolar(\n                r=values,\n                theta=metric_names_closed,\n                fill='toself',\n                name=model_name,\n                hovertemplate='<b>%{theta}</b><br>Score: %{r:.3f}<extra></extra>'\n            ))\n        \n        fig.update_layout(\n            polar=dict(\n                radialaxis=dict(\n                    visible=True,\n                    range=[0, 1]\n                )\n            ),\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 18}\n            },\n            template='plotly_white',\n            height=500\n        )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating model performance chart: {e}\")\n        return go.Figure().add_annotation(\n            text=f\"Error creating performance chart: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n\ndef create_business_metrics_dashboard(metrics_data, title=\"Business Impact Metrics\"):\n    \"\"\"\n    Create a comprehensive dashboard showing key business metrics.\n    \n    Args:\n        metrics_data (dict): Dictionary with various business metrics\n        title (str): Dashboard title\n    \n    Returns:\n        plotly.graph_objects.Figure: Business metrics dashboard\n    \"\"\"\n    try:\n        # Create subplots for different metric types\n        fig = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('Revenue Impact', 'Customer Segments', 'Forecast Accuracy', 'Market Sentiment'),\n            specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                   [{\"type\": \"scatter\"}, {\"type\": \"indicator\"}]]\n        )\n        \n        # Revenue impact (if available)\n        if 'revenue_data' in metrics_data:\n            revenue_data = metrics_data['revenue_data']\n            fig.add_trace(\n                go.Bar(x=list(revenue_data.keys()), y=list(revenue_data.values()), name=\"Revenue\"),\n                row=1, col=1\n            )\n        \n        # Customer segments (if available)\n        if 'segment_data' in metrics_data:\n            segment_data = metrics_data['segment_data']\n            fig.add_trace(\n                go.Pie(labels=list(segment_data.keys()), values=list(segment_data.values()), name=\"Segments\"),\n                row=1, col=2\n            )\n        \n        # Forecast accuracy over time (if available)\n        if 'accuracy_timeline' in metrics_data:\n            accuracy_data = metrics_data['accuracy_timeline']\n            fig.add_trace(\n                go.Scatter(x=accuracy_data['dates'], y=accuracy_data['accuracy'], mode='lines+markers', name=\"Accuracy\"),\n                row=2, col=1\n            )\n        \n        # Market sentiment indicator (if available)\n        if 'sentiment_score' in metrics_data:\n            sentiment_score = metrics_data['sentiment_score']\n            fig.add_trace(\n                go.Indicator(\n                    mode=\"gauge+number\",\n                    value=sentiment_score,\n                    domain={'x': [0, 1], 'y': [0, 1]},\n                    title={'text': \"Sentiment Score\"},\n                    gauge={'axis': {'range': [-1, 1]},\n                           'bar': {'color': \"darkblue\"},\n                           'steps': [{'range': [-1, -0.5], 'color': \"lightgray\"},\n                                     {'range': [-0.5, 0.5], 'color': \"gray\"},\n                                     {'range': [0.5, 1], 'color': \"lightgreen\"}],\n                           'threshold': {'line': {'color': \"red\", 'width': 4},\n                                         'thickness': 0.75, 'value': 0.9}}\n                ),\n                row=2, col=2\n            )\n        \n        fig.update_layout(\n            title={\n                'text': title,\n                'x': 0.5,\n                'xanchor': 'center',\n                'font': {'size': 20}\n            },\n            height=600,\n            template='plotly_white',\n            showlegend=False\n        )\n        \n        return fig\n        \n    except Exception as e:\n        print(f\"Error creating business metrics dashboard: {e}\")\n        return go.Figure().add_annotation(\n            text=f\"Error creating business dashboard: {str(e)}\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False\n        )\n","size_bytes":20104}},"version":1}